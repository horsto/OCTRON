{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "## Horst1 version of Video segmentation with SAM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e50b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### Loading the SAM 2 video predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.build_sam_octron import build_sam2_video_predictor_octron\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "predictor, device  = build_sam2_video_predictor_octron(config_file=model_cfg, \n",
    "                                                       ckpt_path=sam2_checkpoint, \n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97282854",
   "metadata": {},
   "source": [
    "### To do next:\n",
    "- incorporate loading of frames somehow into init state?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbe76695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers.video_loader import get_video_toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8744853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pims import PyAVReaderIndexed\n",
    "# video_file = '/Users/horst/Downloads/bottom-left_03092023_PM-37.mp4'\n",
    "# toclog = get_video_toc(video_file)\n",
    "# toc = toclog['toc']\n",
    "# nframes = toclog['no_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a320d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_pims_indexed = PyAVReaderIndexed(video_file, toc=toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d3c1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_pims_indexed[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5289c909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f23b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b2860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
   "metadata": {},
   "source": [
    "#### Select an example video and implement lazy loading via PIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = '/Users/horst/Downloads/bottom-left_03092023_PM-37.mp4'\n",
    "inference_state = predictor.init_state(video_path=video_dir, \n",
    "                                       offload_video_to_cpu=False,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1f3f6-d74d-4016-934c-8d2a14d1a543",
   "metadata": {},
   "source": [
    "### Example 1: Segment & track one object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state['num_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeb04d-8cba-4f57-95da-6e5a1796003e",
   "metadata": {},
   "source": [
    "#### Step 1: Add a first click on a frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e749bab-0f36-4173-bf8d-0c20cd5214b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (210, 350) to get started\n",
    "points = np.array([[1400, 480]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "#plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[0]> 0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f4731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848d38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a59222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30b025bd-cd58-4bfb-9572-c8d2fd0a02ef",
   "metadata": {},
   "source": [
    "To get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45e932-b0d5-4983-9718-6ee77d1ac31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "    if out_frame_idx > 150: \n",
    "        break\n",
    "# # render the segmentation results every few frames\n",
    "# vis_frame_stride = 20\n",
    "# plt.close(\"all\")\n",
    "# for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     plt.title(f\"frame {out_frame_idx}\")\n",
    "#     plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "#     for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "#         show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023f91f-0cc5-4980-ae8e-a13c5749112b",
   "metadata": {},
   "source": [
    "Note that in addition to clicks or boxes, SAM 2 also supports directly using a **mask prompt** as input via the `add_new_mask` method in the `SAM2VideoPredictor` class. This can be helpful in e.g. semi-supervised VOS evaluations (see [tools/vos_inference.py](https://github.com/facebookresearch/sam2/blob/main/tools/vos_inference.py) for an example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da018be8-a4ae-4943-b1ff-702c2b89cb68",
   "metadata": {},
   "source": [
    "### Example 3: Segment multiple objects simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6c04c-3072-4876-b394-879321a48c4a",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29b874c8-9f39-42d3-a667-54a0bd696410",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3f7e6-4821-468c-84e4-f3a0435c9149",
   "metadata": {},
   "source": [
    "#### Step 1: Add two objects on a frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95158714-86d7-48a9-8365-b213f97cc9ca",
   "metadata": {},
   "source": [
    "SAM 2 can also segment and track two or more objects at the same time. One way, of course, is to do them one by one. However, it would be more efficient to batch them together (e.g. so that we can share the image features between objects to reduce computation costs).\n",
    "\n",
    "This time, let's focus on object parts and segment **the shirts of both childen** in this video. Here we add prompts for these two objects and assign each of them a unique object id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e22d896d-3cd5-4fa0-9230-f33e217035dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {}  # hold all the clicks we add for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9ac57-b14a-4237-828d-927e422c518b",
   "metadata": {},
   "source": [
    "Add the first object (the left child's shirt) with a **positive click** at (x, y) = (200, 300) on frame 0.\n",
    "\n",
    "We assign it to object id `2` (it can be arbitrary integers, and only needs to be unique for each object to track), which is passed to the `add_new_points_or_box` API to distinguish the object we are clicking upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13432fc-f467-44d8-adfe-3e0c488046b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (200, 300) to get started on the first object\n",
    "points = np.array([[200, 300]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbd51b-e1e2-4c36-99ec-1d9a1b49b0cd",
   "metadata": {},
   "source": [
    "Hmm, this time we just want to select the child's shirt, but the model predicts the mask for the entire child. Let's refine the prediction with a **negative click** at (x, y) = (275, 175)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecf61d-662b-4f98-ae62-46557b219842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the first object\n",
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a 2nd negative click at (x, y) = (275, 175) to refine the first object\n",
    "# sending all clicks (and their labels) to `add_new_points_or_box`\n",
    "points = np.array([[200, 300], [275, 175]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1, 0], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194718c1-734d-446c-a3ef-361057de2f31",
   "metadata": {},
   "source": [
    "After the 2nd negative click, now we get the left child's shirt as our first object.\n",
    "\n",
    "Let's move on to the second object (the right child's shirt) with a positive click at (x, y) = (400, 150) on frame 0. Here we assign object id `3` to this second object (it can be arbitrary integers, and only needs to be unique for each object to track).\n",
    "\n",
    "Note: when there are multiple objects, the `add_new_points_or_box` API will return a list of masks for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca1bde-62a4-40e6-98e4-15606441e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 3  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's now move on to the second object we want to track (giving it object id `3`)\n",
    "# with a positive click at (x, y) = (400, 150)\n",
    "points = np.array([[400, 150]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "# `add_new_points_or_box` returns masks for all objects added so far on this interacted frame\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame on all objects\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7add8-d577-4597-ae2f-654b8c7b05e0",
   "metadata": {},
   "source": [
    "This time the model predicts the mask of the shirt we want to track in just one click. Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448733b8-ea8b-4078-995f-b676c3b558ba",
   "metadata": {},
   "source": [
    "#### Step 2: Propagate the prompts to get masklets across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd73de-d669-41c8-b6ba-943883f0caa2",
   "metadata": {},
   "source": [
    "Now, we propagate the prompts for both objects to get their masklets throughout the video.\n",
    "\n",
    "Note: when there are multiple objects, the `propagate_in_video` API will return a list of masks for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17737191-d62b-4611-b2c6-6d0418a9ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0b9d7-c78f-432b-afb0-11f2ea5b652a",
   "metadata": {},
   "source": [
    "Looks like both children's shirts are well segmented in this video.\n",
    "\n",
    "Now you can try SAM 2 on your own videos and use cases! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
