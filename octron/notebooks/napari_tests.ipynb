{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Napari tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "cur_path = Path(os.getcwd()).parent\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import cmasher as cmr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='white')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "from napari.qt.threading import thread_worker\n",
    "import warnings\n",
    "warnings.simplefilter(action='always', category=FutureWarning)\n",
    "import zarr\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 16:30:31.035 python[33449:14995171] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-29 16:30:31.035 python[33449:14995171] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the SAM2 model like you do in Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import torch\n",
    "from octron.sam2_octron.helpers.build_sam2_octron import build_sam2_octron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Support for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\n",
      "Uing device: mps\n",
      "\n",
      "\n",
      "Loaded SAM2VideoPredictor OCTRON\n",
      "Model image size: 1024\n"
     ]
    }
   ],
   "source": [
    "sam2_folder = Path('sam2_octron')\n",
    "checkpoint = 'sam2.1_hiera_base_plus.pt' # under folder /checkpoints\n",
    "model_cfg = 'sam2.1/sam2.1_hiera_b+.yaml' # under folder /configs\n",
    "# ------------------------------------------------------------------------------------\n",
    "sam2_checkpoint = cur_path / sam2_folder / Path(f'checkpoints/{checkpoint}')\n",
    "model_cfg = Path(f'configs/{model_cfg}')\n",
    "\n",
    "\n",
    "predictor, device  = build_sam2_octron(config_file=model_cfg.as_posix(), \n",
    "                                                       ckpt_path=sam2_checkpoint.as_posix(), \n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From napari, after loading video file, extract info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 771, 771)\n"
     ]
    }
   ],
   "source": [
    "viewer.dims.set_point(0,0)\n",
    "current_indices = viewer.dims.current_step\n",
    "print(current_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video shape: (10801, 1544, 1544, 3)\n"
     ]
    }
   ],
   "source": [
    "video_data = viewer.layers[0].data   # the whole video\n",
    "\n",
    "print(f'video shape: {video_data.shape}')   \n",
    "num_frames, height, width, channels = video_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zarr store to save all resized images \n",
    "chunk_size = 25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create temp output dir \n",
    "sample_dir = cur_path / 'sample_data'\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "sample_data_zarr = sample_dir / 'sample_data.zarr'\n",
    "if sample_data_zarr.exists():\n",
    "    shutil.rmtree(sample_data_zarr)\n",
    "\n",
    "# Assuming local store on fast SSD, so no compression employed for now \n",
    "store = zarr.storage.LocalStore(sample_data_zarr, read_only=False)\n",
    "root = zarr.create_group(store=store)\n",
    "image_zarr = root.create_array(name='masks',\n",
    "                                shape=(num_frames, 3, predictor.image_size, predictor.image_size),  \n",
    "                                chunks=(chunk_size, 3, predictor.image_size, predictor.image_size), \n",
    "                                fill_value=np.nan,\n",
    "                                dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/torch/nn/functional.py:4594: UserWarning: The operator 'aten::upsample_bicubic2d.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  return torch._C._nn.upsample_bicubic2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized SAM2 model\n"
     ]
    }
   ],
   "source": [
    "predictor.init_state(napari_viewer=viewer, video_layer_idx=0, zarr_store=image_zarr)\n",
    "predictor.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_new_pred(frame_idx,\n",
    "                 obj_id, \n",
    "                 label,\n",
    "                 point):\n",
    "    \n",
    "    assert label in [0,1]\n",
    "    # Run prediction\n",
    "    #obj_id : give a unique id to each object we interact with (it can be any integers)\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "                                                frame_idx=frame_idx,\n",
    "                                                obj_id=obj_id,\n",
    "                                                points=np.array([point],dtype=np.float32),\n",
    "                                                labels=np.array([label], np.int32)\n",
    "                                                )\n",
    "    \n",
    "    # Add the mask image as a new labels layer\n",
    "    mask = (out_mask_logits[0] > 0).cpu().numpy().astype(np.uint8)\n",
    "    current_label = obj_id+1\n",
    "    if len(np.unique(mask))>1:\n",
    "        mask[mask==np.unique(mask)[1]] = current_label \n",
    "    mask = mask.squeeze()\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up thread worker to deal with prefetching batches of images\n",
    "\n",
    "@thread_worker\n",
    "def thread_prefetch_images(batch_size):\n",
    "    global viewer\n",
    "    current_indices = viewer.dims.current_step\n",
    "    _ = predictor.images[slice(current_indices[0],current_indices[0]+chunk_size)]\n",
    "prefetcher_worker = thread_prefetch_images(chunk_size)   \n",
    "prefetcher_worker.setAutoDelete(False)\n",
    "prefetcher_worker.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the mask and annotation layers \n",
    "# Keep them empty at start \n",
    "mask_layer_dummy = np.zeros((num_frames, height, width), dtype=np.uint8)\n",
    "mask_layer_dummy.shape\n",
    "\n",
    "colors = cmr.take_cmap_colors('cmr.tropical', 8, cmap_range=(0, 1),\n",
    "                     return_fmt='int')\n",
    "colors_norm = np.stack(colors) / 255.0  \n",
    "cyclic_map = napari.utils.CyclicLabelColormap(\n",
    "    np.hstack([np.stack(colors_norm), np.ones((len(colors), 1))])\n",
    ")\n",
    "labels_layer = viewer.add_labels(\n",
    "    mask_layer_dummy, \n",
    "    name='Mask',  # Name of the layer\n",
    "    opacity=0.4,  # Optional: opacity of the labels\n",
    "    colormap=cyclic_map,\n",
    "    blending='additive'  # Optional: blending mode\n",
    ")\n",
    "# Add the points layer to the viewer\n",
    "points_layer = viewer.add_points(None, \n",
    "                                 ndim=3,\n",
    "                                 name='Annotations', \n",
    "                                 scale=(1,1),\n",
    "                                 size=40,\n",
    "                                 border_color='dimgrey',\n",
    "                                 border_width=.2,\n",
    "                                 opacity=.6,\n",
    "                                 )\n",
    "# Store the initial length of the points data\n",
    "previous_length_points = len(points_layer.data)\n",
    "\n",
    "\n",
    "left_right_click = 'left'\n",
    "def on_mouse_press(layer, event):\n",
    "    '''\n",
    "    Generic function to catch left and right mouse clicks\n",
    "    '''\n",
    "    global left_right_click\n",
    "    if event.type == 'mouse_press':\n",
    "        if event.button == 1:  # Left-click\n",
    "            left_right_click = 'left'\n",
    "        elif event.button == 2:  # Right-click\n",
    "            left_right_click = 'right'     \n",
    "    \n",
    "\n",
    "def on_points_added(event):\n",
    "    '''\n",
    "    Function to run when points are added to the points layer\n",
    "    '''\n",
    "    \n",
    "    global points_layer\n",
    "    global labels_layer\n",
    "    global left_right_click\n",
    "    global prefetcher_worker\n",
    "    global previous_length_points\n",
    "    \n",
    "    current_length = len(points_layer.data)\n",
    "    if current_length > previous_length_points:\n",
    "        previous_length_points = current_length \n",
    "\n",
    "        # # Execute prediction \n",
    "        newest_point_data =  points_layer.data[-1]\n",
    "        if left_right_click == 'left':\n",
    "            label = 1\n",
    "            points_layer.face_color[-1] = [0.59607846, 0.98431373, 0.59607846, 1.]\n",
    "            points_layer.symbol[-1] = 'o'\n",
    "        elif left_right_click == 'right':\n",
    "            label = 0\n",
    "            points_layer.face_color[-1] = [1., 1., 1., 1.]\n",
    "            points_layer.symbol[-1] = 'x'\n",
    "        points_layer.refresh() \n",
    "        # Run prediction\n",
    "        frame_idx  = int(newest_point_data[0])\n",
    "        point_data = newest_point_data[1:][::-1]\n",
    "        mask = run_new_pred(frame_idx=frame_idx,\n",
    "                            obj_id=0,\n",
    "                            label=label,\n",
    "                            point=point_data,\n",
    "                            )\n",
    "        labels_layer.data[frame_idx,:,:] = mask\n",
    "        labels_layer.refresh()   \n",
    "        \n",
    "        # Prefetch batch of images\n",
    "        # This is done here since adding it as direct mouse interaction \n",
    "        # slows down the first prediction\n",
    "        if not prefetcher_worker.is_running:\n",
    "            prefetcher_worker.run()\n",
    "\n",
    "\n",
    "points_layer.mouse_drag_callbacks.append(on_mouse_press)\n",
    "points_layer.events.data.connect(on_points_added)\n",
    "\n",
    "# Hide the transform, delete, and select buttons\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[points_layer]\n",
    "getattr(qctrl, 'transform_button').setVisible(False)\n",
    "getattr(qctrl, 'delete_button').setVisible(False)\n",
    "getattr(qctrl, 'select_button').setVisible(False)\n",
    "# Select the current, add tool for the points layer\n",
    "viewer.layers.selection.active = points_layer\n",
    "viewer.layers.selection.active.mode = 'add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the shapes layer to the viewer\n",
    "shapes_layer = viewer.add_shapes(None, \n",
    "                                 ndim=3,\n",
    "                                 name='Shape annotations', \n",
    "                                 scale=(1,1),\n",
    "                                 edge_width=0,\n",
    "                                 face_color=np.array(cyclic_map.colors),\n",
    "                                 opacity=.6,\n",
    "                                 )\n",
    "# Store the initial length of the points data\n",
    "previous_length_points = len(shapes_layer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_id = 0\n",
    "\n",
    "\n",
    "@thread_worker\n",
    "def thread_predict(frame_idx, max_imgs):\n",
    "    global labels_layer\n",
    "    video_segments = {} \n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(start_frame_idx=frame_idx, \n",
    "                                                                                    max_frame_num_to_track=max_imgs):\n",
    "        \n",
    "        print('predicted')\n",
    "        for i, out_obj_id in enumerate(out_obj_ids):\n",
    "            video_segments[out_frame_idx] = {out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()}\n",
    "            \n",
    "        #  Add the mask image as a new labels layer\n",
    "        mask = video_segments[out_frame_idx][obj_id] # THIS NEEDS TO BE MADE LAYER SPECIFIC \n",
    "        current_label = obj_id+1\n",
    "        if len(np.unique(mask))>1:\n",
    "            mask[mask==np.unique(mask)[1]] = current_label \n",
    "            \n",
    "            \n",
    "        mask = mask.squeeze()\n",
    "        labels_layer.data[out_frame_idx+frame_idx,:,:]= mask\n",
    "        viewer.dims.set_point(0,out_frame_idx+frame_idx)\n",
    "        labels_layer.refresh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non conditioned frame 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/26 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected query, key, and value to have the same dtype, but got query.dtype: float key.dtype: float and value.dtype: c10::BFloat16 instead.\n"
     ]
    }
   ],
   "source": [
    "worker = thread_predict(frame_idx=viewer.dims.current_step[0] , max_imgs=chunk_size)  # create \"worker\" object\n",
    "#worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "worker.start()  # start the thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker.quit()  # stop the thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict_per_obj is huge \n",
    "# Structure\n",
    "# -> obj_id\n",
    "# --> cond_frame_outputs\n",
    "# --> non_cond_frame_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_snippet = video_data[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "_resize_img = Resize(\n",
    "                        size=(predictor.image_size) # This is 1024x1024 for the l model\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_snippet = video_data[100:150]  \n",
    "data_snippet.shape\n",
    "\n",
    "data_snippet_torch = _resize_img(torch.from_numpy(data_snippet).permute(0,3,1,2)).float()\n",
    "data_snippet_torch /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mean = (0.485, 0.456, 0.406)\n",
    "img_std  = (0.229, 0.224, 0.225)\n",
    "img_mean = torch.tensor(img_mean, dtype=torch.float32)[:, None, None]\n",
    "img_std = torch.tensor(img_std, dtype=torch.float32)[:, None, None]\n",
    "\n",
    "data_snippet_torch -= img_mean\n",
    "data_snippet_torch /= img_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_snippet_torch -= img_mean\n",
    "data_snippet_torch /= img_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_snippet_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data_snippet_torch[0][2,:,:].numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
