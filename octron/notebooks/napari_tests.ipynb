{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Napari tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "cur_path = Path(os.getcwd()).parent\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import cmasher as cmr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='white')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "from napari.qt.threading import thread_worker\n",
    "from napari.utils import DirectLabelColormap\n",
    "import warnings\n",
    "warnings.simplefilter(action='always', category=FutureWarning)\n",
    "import zarr\n",
    "import time\n",
    "\n",
    "#### Importing additional stuff \n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the SAM2 model like you do in Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "from octron.sam2_octron.helpers.build_sam2_octron import build_sam2_octron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam2_folder = Path('sam2_octron')\n",
    "checkpoint = 'sam2.1_hiera_base_plus.pt' # under folder /checkpoints\n",
    "model_cfg = 'sam2.1/sam2.1_hiera_b+.yaml' # under folder /configs\n",
    "# ----------------------------------------------------------------------------\n",
    "sam2_checkpoint = cur_path / sam2_folder / Path(f'checkpoints/{checkpoint}')\n",
    "model_cfg = Path(f'configs/{model_cfg}')\n",
    "\n",
    "\n",
    "predictor, device  = build_sam2_octron(config_file=model_cfg.as_posix(), \n",
    "                                       ckpt_path=sam2_checkpoint.as_posix(), \n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From napari, after loading video file, extract info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.dims.set_point(0,0)\n",
    "current_indices = viewer.dims.current_step\n",
    "print(current_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = viewer.layers[0].data   # the whole video\n",
    "\n",
    "print(f'video shape: {video_data.shape}')   \n",
    "num_frames, height, width, channels = video_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zarr store to save all resized images \n",
    "chunk_size = 20\n",
    "\n",
    "\n",
    "\n",
    "# Create temp output dir \n",
    "sample_dir = cur_path / 'sample_data'\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "sample_data_zarr = sample_dir / 'sample_data.zip'\n",
    "if sample_data_zarr.exists():\n",
    "    os.remove(sample_data_zarr)\n",
    "\n",
    "# Assuming local store on fast SSD, so no compression employed for now \n",
    "store = zarr.storage.ZipStore(sample_data_zarr, mode='w')\n",
    "root = zarr.create_group(store=store)\n",
    "image_zarr = root.create_array(name='masks',\n",
    "                                shape=(num_frames, 3, predictor.image_size, predictor.image_size),  \n",
    "                                chunks=(chunk_size, 3, predictor.image_size, predictor.image_size), \n",
    "                                fill_value=np.nan,\n",
    "                                dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.init_state(napari_viewer=viewer, video_layer_idx=0, zarr_store=image_zarr)\n",
    "predictor.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to load previous checkpoint does not work \n",
    "# Since the model still expects at least one input\n",
    "\n",
    "# import torch \n",
    "# state_path = '/Users/horst/Documents/python/OCTRON/octron/sample_data/model_output.pth'\n",
    "# checkpoint = torch.load(state_path, weights_only=True)\n",
    "# predictor.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_new_pred(frame_idx,\n",
    "                 obj_id, \n",
    "                 label,\n",
    "                 point):\n",
    "    \n",
    "    assert label in [0,1]\n",
    "    # Run prediction\n",
    "    #obj_id : give a unique id to each object we interact with (it can be any integers)\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "                                                frame_idx=frame_idx,\n",
    "                                                obj_id=obj_id,\n",
    "                                                points=np.array([point],dtype=np.float32),\n",
    "                                                labels=np.array([label], np.int32)\n",
    "                                                )\n",
    "    \n",
    "    # Add the mask image as a new labels layer\n",
    "    mask = (out_mask_logits[0] > 0).cpu().numpy().astype(np.uint8)\n",
    "    current_label = obj_id+1\n",
    "    if len(np.unique(mask))>1:\n",
    "        mask[mask==np.unique(mask)[1]] = current_label \n",
    "    mask = mask.squeeze()\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up thread worker to deal with prefetching batches of images\n",
    "@thread_worker\n",
    "def thread_prefetch_images(batch_size):\n",
    "    print('running prefetcher')\n",
    "    global viewer\n",
    "    current_indices = viewer.dims.current_step\n",
    "    _ = predictor.images[slice(current_indices[0],current_indices[0]+batch_size)]\n",
    "prefetcher_worker = thread_prefetch_images(chunk_size)   \n",
    "prefetcher_worker.setAutoDelete(False)\n",
    "prefetcher_worker.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a kind of \"global\" colormap for all labels\n",
    "# Idea:\n",
    "# - create a continous colormap and cmap_range for each label \n",
    "# --> this way we get sub colormaps and for the same label name can cycle through these \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_colors(cmap='cmr.tropical', label_n=4, obj_n=5):\n",
    "    '''\n",
    "    Create color map dictionary for labels \n",
    "    label(int) -> color list -> color(4D)\n",
    "    \n",
    "    For each label (n=label_cat_n) create a sub colormap with color_cat_n colors.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    label_cat_rel = np.linspace(0,1,label_n+1) # This must be the ugliest written fctn in the world\n",
    "\n",
    "    label_colors = {}\n",
    "    for cat in range(len(label_cat_rel)-1):\n",
    "        rel_range = label_cat_rel[cat:cat+2]\n",
    "        colors = np.array(cmr.take_cmap_colors(cmap, \n",
    "                                              obj_n, \n",
    "                                              cmap_range=(rel_range[0], rel_range[1]), \n",
    "                                              return_fmt='int'\n",
    "                                              ) \n",
    "                        ) / 255.0\n",
    "        colors4d = np.hstack([colors, np.ones((len(colors), 1))])\n",
    "        label_colors[cat] = {i+1: color for i, color in enumerate(colors4d)} # Keys start at 1 !\n",
    "        label_colors[cat][None] = np.array([0,0,0,0]).astype(np.float32)\n",
    "    return label_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 2 # Only plays a role here ... not in predictor  \n",
    "obj_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the mask and annotation layers \n",
    "# Keep them empty at start \n",
    "mask_layer_dummy = np.zeros((num_frames, height, width), dtype=np.uint8)\n",
    "mask_layer_dummy.shape\n",
    "\n",
    "\n",
    "# Select colormap for labels layer based on category (label) and current object ID \n",
    "current_color_labelmap = DirectLabelColormap(color_dict=colors[label_id], \n",
    "                                             use_selection=True, \n",
    "                                             selection=obj_id+1,\n",
    "                                             )\n",
    "labels_layer = viewer.add_labels(\n",
    "    mask_layer_dummy, \n",
    "    name='Mask',  # Name of the layer\n",
    "    opacity=0.4,  # Optional: opacity of the labels\n",
    "    blending='additive',  # Optional: blending mode\n",
    "    colormap=current_color_labelmap, \n",
    ")\n",
    "# Add the points layer to the viewer\n",
    "points_layer = viewer.add_points(None, \n",
    "                                 ndim=3,\n",
    "                                 name='Annotations', \n",
    "                                 scale=(1,1),\n",
    "                                 size=40,\n",
    "                                 border_color='dimgrey',\n",
    "                                 border_width=.2,\n",
    "                                 opacity=.6,\n",
    "                                 )\n",
    "# Store the initial length of the points data\n",
    "previous_length_points = len(points_layer.data)\n",
    "\n",
    "\n",
    "left_right_click = 'left'\n",
    "def on_mouse_press(layer, event):\n",
    "    '''\n",
    "    Generic function to catch left and right mouse clicks\n",
    "    '''\n",
    "    global left_right_click\n",
    "    if event.type == 'mouse_press':\n",
    "        if event.button == 1:  # Left-click\n",
    "            left_right_click = 'left'\n",
    "        elif event.button == 2:  # Right-click\n",
    "            left_right_click = 'right'     \n",
    "    \n",
    "\n",
    "def on_points_added(event):\n",
    "    '''\n",
    "    Function to run when points are added to the points layer\n",
    "    '''\n",
    "    \n",
    "    global points_layer\n",
    "    global labels_layer\n",
    "    global left_right_click\n",
    "    global prefetcher_worker\n",
    "    global previous_length_points\n",
    "    \n",
    "    current_length = len(points_layer.data)\n",
    "    if current_length > previous_length_points:\n",
    "        previous_length_points = current_length \n",
    "\n",
    "        # # Execute prediction \n",
    "        newest_point_data =  points_layer.data[-1]\n",
    "        if left_right_click == 'left':\n",
    "            label = 1\n",
    "            points_layer.face_color[-1] = [0.59607846, 0.98431373, 0.59607846, 1.]\n",
    "            points_layer.symbol[-1] = 'o'\n",
    "        elif left_right_click == 'right':\n",
    "            label = 0\n",
    "            points_layer.face_color[-1] = [1., 1., 1., 1.]\n",
    "            points_layer.symbol[-1] = 'x'\n",
    "        points_layer.refresh() \n",
    "        # Run prediction\n",
    "        frame_idx  = int(newest_point_data[0])\n",
    "        point_data = newest_point_data[1:][::-1]\n",
    "        mask = run_new_pred(frame_idx=frame_idx,\n",
    "                            obj_id=0,\n",
    "                            label=label,\n",
    "                            point=point_data,\n",
    "                            )\n",
    "        labels_layer.data[frame_idx,:,:] = mask\n",
    "        labels_layer.refresh()   \n",
    "        \n",
    "        # Prefetch batch of images\n",
    "        # This is done here since adding it as direct mouse interaction \n",
    "        # slows down the first prediction\n",
    "        if not prefetcher_worker.is_running:\n",
    "            prefetcher_worker.run()\n",
    "\n",
    "\n",
    "points_layer.mouse_drag_callbacks.append(on_mouse_press)\n",
    "points_layer.events.data.connect(on_points_added)\n",
    "\n",
    "# Hide the transform, delete, and select buttons\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[points_layer]\n",
    "getattr(qctrl, 'transform_button').setVisible(False)\n",
    "getattr(qctrl, 'delete_button').setVisible(False)\n",
    "getattr(qctrl, 'select_button').setVisible(False)\n",
    "# Select the current, add tool for the points layer\n",
    "viewer.layers.selection.active = points_layer\n",
    "viewer.layers.selection.active.mode = 'add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the shapes layer to the viewer\n",
    "\n",
    "shapes_layer = viewer.add_shapes(None, \n",
    "                                 ndim=3,\n",
    "                                 name='Shape annotations', \n",
    "                                 scale=(1,1),\n",
    "                                 edge_width=0,\n",
    "                                 face_color=colors[label_id][obj_id+1],\n",
    "                                 opacity=.6,\n",
    "                                 )\n",
    "# Store the initial length of the points data\n",
    "# previous_length_points = len(shapes_layer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_id = 0\n",
    "\n",
    "\n",
    "@thread_worker\n",
    "def thread_predict(frame_idx, max_imgs):\n",
    "    global labels_layer\n",
    "\n",
    "    video_segments = {} \n",
    "    start_time = time.time()\n",
    "    # Prefetch images if they are not cached yet \n",
    "    _ = predictor.images[slice(frame_idx,frame_idx+max_imgs)]\n",
    "    \n",
    "    # Loop over frames and run prediction (single frame!)\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(start_frame_idx=frame_idx, \n",
    "                                                                                    max_frame_num_to_track=max_imgs):\n",
    "        \n",
    "        for i, out_obj_id in enumerate(out_obj_ids):\n",
    "            video_segments[out_frame_idx] = {out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()}\n",
    "            if not out_obj_id in predictor.inference_state['centroids']:\n",
    "                predictor.inference_state['centroids'][out_obj_id] = {}\n",
    "            if not out_obj_id in predictor.inference_state['areas']:\n",
    "                predictor.inference_state['areas'][out_obj_id] = {}\n",
    "                \n",
    "        # PICK ONE OBJ (OBJ_ID = 0 or whatever)\n",
    "        \n",
    "        #  Add the mask image as a new labels layer\n",
    "        mask = video_segments[out_frame_idx][obj_id] # THIS NEEDS TO BE MADE LAYER SPECIFIC \n",
    "        current_label = obj_id+1\n",
    "        if len(np.unique(mask))>1:\n",
    "            mask[mask==np.unique(mask)[1]] = current_label \n",
    "\n",
    "        mask = mask.squeeze()\n",
    "        props = measure.regionprops(mask.astype(int))[0]\n",
    "        predictor.inference_state['centroids'][obj_id][out_frame_idx] = props.centroid\n",
    "        predictor.inference_state['areas'][obj_id][out_frame_idx] = props.area\n",
    "        labels_layer.data[out_frame_idx,:,:] = mask\n",
    "        viewer.dims.set_point(0,out_frame_idx)\n",
    "        labels_layer.refresh()\n",
    "    end_time = time.time()\n",
    "    print(f'start idx {frame_idx} | {max_imgs} frames in {end_time-start_time} s')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker = thread_predict(frame_idx=viewer.dims.current_step[0], max_imgs=chunk_size) \n",
    "#worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "worker.start()  # start the thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tried to save the checkpoint, \n",
    "# # but this does not work. \n",
    "# # the check point model_state does not contain enough info \n",
    "# import torch\n",
    "# model_output_path = sample_dir / 'model_output.pth'    \n",
    "# torch.save({\n",
    "#             'model_state_dict': predictor.state_dict(),\n",
    "#             }, model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the whole video as test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test \n",
    "# for i in range(0,4000,chunk_size):\n",
    "    \n",
    "#     prediction_worker = thread_predict(frame_idx=i, max_imgs=chunk_size)  \n",
    "#     prediction_worker.setAutoDelete(True)\n",
    "#     #worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "#     prediction_worker.start()  \n",
    "#     print(f'Highest cached index {int(np.nanmax(predictor.images.cached_indices))}')\n",
    "#     time.sleep(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting\n",
    "# import seaborn as sns\n",
    "# sns.set(style='white')\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "# from matplotlib import pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "# mpl.rcParams.update({\"axes.grid\" : True, \"grid.color\": \"grey\", \"grid.alpha\": .1})\n",
    "# plt.rcParams['xtick.major.size'] = 10\n",
    "# plt.rcParams['xtick.major.width'] = 1\n",
    "# plt.rcParams['ytick.major.size'] = 10\n",
    "# plt.rcParams['ytick.major.width'] = 1\n",
    "# plt.rcParams['xtick.bottom'] = True\n",
    "# plt.rcParams['ytick.left'] = True\n",
    "# mpl.rcParams['savefig.pad_inches'] = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the centroids over time\n",
    "# centroids = list(predictor.inference_state['centroids'][0].values())\n",
    "# centroids = np.stack(centroids)\n",
    "# areas = np.array(list(predictor.inference_state['areas'][0].values())).astype(float)\n",
    "# figure = plt.figure(figsize=(10,10))\n",
    "# plt.imshow(viewer.layers[0].data[0], cmap='gray')\n",
    "# #plt.plot(centroids[:,1], centroids[:,0], '-', color='k', alpha=.6)   \n",
    "# plt.scatter(centroids[:,1], centroids[:,0], s=areas/50, marker='.', color='pink', alpha=.15, lw=0)   \n",
    "# sns.despine(left=True,bottom=True)\n",
    "# plt.title(f'Centroids over time (n={centroids.shape[0]} frames)')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(predictor.inference_state['areas'][0].values()),'-', color='w', alpha=.6 )\n",
    "# plt.title('Area over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict_per_obj is huge \n",
    "# Structure\n",
    "# -> obj_id\n",
    "# --> cond_frame_outputs\n",
    "# --> non_cond_frame_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
