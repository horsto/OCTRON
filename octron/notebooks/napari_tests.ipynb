{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Napari tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "cur_path = Path(os.getcwd()).parent\n",
    "sam2_path = cur_path / 'sam2_octron'\n",
    "sys.path.append(cur_path.as_posix())\n",
    "from matplotlib import pyplot as plt\n",
    "import cmasher as cmr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='white')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "from napari.qt.threading import thread_worker\n",
    "from napari.utils import DirectLabelColormap\n",
    "from napari.utils.notifications import show_info\n",
    "import warnings\n",
    "warnings.simplefilter(action='always', category=FutureWarning)\n",
    "import time\n",
    "\n",
    "#### Importing additional stuff \n",
    "from skimage import measure\n",
    "from skimage.draw import polygon2mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the SAM2 model like you do in Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2_octron.helpers.sam2_checks import check_model_availability\n",
    "from sam2_octron.helpers.sam2_octron import run_new_pred\n",
    "from sam2_octron.helpers.build_sam2_octron import build_sam2_octron\n",
    "from sam2_octron.helpers.sam2_zarr import create_image_zarr\n",
    "from sam2_octron.helpers.sam2_colors import create_label_colors\n",
    "from sam2_octron.helpers import get_file_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_yaml_path = sam2_path / 'models.yaml'\n",
    "models_dict = check_model_availability(SAM2p1_BASE_URL='',\n",
    "                                       models_yaml_path=models_yaml_path,\n",
    "                                      force_download=False,\n",
    "                                      )\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Careful . these path descriptors differ slightly between notebook and \n",
    "# plugin version\n",
    "model = models_dict['sam2_large']\n",
    "\n",
    "config_path = Path(model['config_path'])\n",
    "checkpoint_path = sam2_path / Path(f\"{model['checkpoint_path']}\")\n",
    "predictor, device  = build_sam2_octron(config_file=config_path.as_posix(), \n",
    "                                       ckpt_path=checkpoint_path.as_posix(), \n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From napari, after loading video file, extract info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.dims.set_point(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_video_layer(viewer):\n",
    "    '''\n",
    "    Find the video layer in the napari viewer\n",
    "    and attach useful info to it. \n",
    "    \n",
    "    '''\n",
    "    image_layers = []\n",
    "    for l in viewer.layers:\n",
    "        if l._basename() == 'Image':\n",
    "            l.editable = False\n",
    "            layer_dict = l.source.dict()\n",
    "            layer_dict['id'] = l.unique_id\n",
    "            layer_dict['data'] = l.data\n",
    "            layer_dict['shortened_name'] = '...' + l.name[-10:]\n",
    "            layer_dict['num_frames'] = l.data.shape[0]\n",
    "            layer_dict['height'] = l.data.shape[1]\n",
    "            layer_dict['width'] = l.data.shape[2]\n",
    "            layer_dict['nchannels'] = l.data.shape[3]\n",
    "            layer_dict['hash'] = get_file_hash(layer_dict['path'])\n",
    "            image_layers.append(layer_dict)\n",
    "    assert len(image_layers) == 1, 'You need one layer with video data exactly'\n",
    "    image_layer = image_layers[0]\n",
    "    # Create a retrievable mask dummy\n",
    "    mask_layer_dummy = np.zeros((layer_dict['num_frames'], \n",
    "                                 layer_dict['height'], \n",
    "                                 layer_dict['width']\n",
    "                                 ), dtype=np.uint8\n",
    "                                )\n",
    "    mask_layer_dummy.shape\n",
    "    image_layer['mask_dummy'] = mask_layer_dummy    \n",
    "    return image_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_layer = find_video_layer(viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect some metadata for the loaded video\n",
    "num_frames = image_layer['num_frames']\n",
    "video_height = image_layer['height']\n",
    "video_width = image_layer['width']\n",
    "video_nchannels = image_layer['nchannels']\n",
    "video_data = image_layer['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp output dir \n",
    "sample_dir = cur_path / 'sample_data'\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "sample_data_zarr = sample_dir / 'sample_data.zip'\n",
    "\n",
    "image_zarr = create_image_zarr(sample_data_zarr,\n",
    "                               num_frames=num_frames,\n",
    "                               image_height=predictor.image_size,\n",
    "                               chunk_size=chunk_size,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.init_state(video_data=video_data, zarr_store=image_zarr)\n",
    "predictor.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up thread worker to deal with prefetching batches of images\n",
    "@thread_worker\n",
    "def thread_prefetch_images(batch_size):\n",
    "    global viewer\n",
    "    current_indices = viewer.dims.current_step\n",
    "    print(f'Prefetching {batch_size} images, starting at frame {current_indices[0]}')\n",
    "    _ = predictor.images[slice(current_indices[0],current_indices[0]+batch_size)]\n",
    "prefetcher_worker = thread_prefetch_images(chunk_size)   \n",
    "prefetcher_worker.setAutoDelete(False)\n",
    "prefetcher_worker.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = create_label_colors(cmap='cmr.tropical')\n",
    "# Select colormap for labels layer based on category (label) and current object ID \n",
    "base_color = DirectLabelColormap(color_dict=colors[0], \n",
    "                                 use_selection=True, \n",
    "                                 selection=1,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement layer remove events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def on_layer_removed(event):\n",
    "#     global viewer\n",
    "#     global remove_current_layer, removed_layer\n",
    "#     print('Calling on_layer_removed')\n",
    "#     # if not remove_current_layer:\n",
    "#     #     viewer.add_layer(removed_layer)\n",
    "#     # else:\n",
    "#     #     print(f\"Deleted layer {removed_layer}\")\n",
    "        \n",
    "# def on_layer_removing(event):\n",
    "#     global remove_current_layer, removed_layer\n",
    "    \n",
    "#     layer2remove = event.source[event.index]\n",
    "#     # Not sure if possible to delete more than one\n",
    "#     # IF so, then take care of it ... event.sources is as list\n",
    "    \n",
    "#     reply = QMessageBox.question(\n",
    "#         None, \n",
    "#         \"Confirmation\", \n",
    "#         f\"Are you sure you want to delete layer\\n'{layer2remove}'\",\n",
    "#         QMessageBox.Yes | QMessageBox.No,\n",
    "#         QMessageBox.No\n",
    "#     )\n",
    "#     if reply == QMessageBox.No:\n",
    "#         remove_current_layer = False\n",
    "#         removed_layer = layer2remove\n",
    "#     else:\n",
    "#         remove_current_layer = True\n",
    "        \n",
    "# viewer.layers.events.removing.connect(on_layer_removing)\n",
    "# viewer.layers.events.removed.connect(on_layer_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_layer = viewer.add_labels(\n",
    "    image_layer['mask_dummy'], \n",
    "    name='SAM2 masks',  \n",
    "    opacity=0.4,  \n",
    "    blending='additive',  \n",
    "    colormap=base_color, \n",
    ")\n",
    "\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[labels_layer]\n",
    "buttons_to_hide = ['erase_button',\n",
    "                   'fill_button',\n",
    "                   'paint_button',\n",
    "                   'pick_button',\n",
    "                   'polygon_button',\n",
    "                   'transform_button',\n",
    "                   ]\n",
    "for btn in buttons_to_hide: \n",
    "    getattr(qctrl, btn).hide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapes layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = 'octOus '\n",
    "label_name = label_name.strip().lower()\n",
    "current_color = colors[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the shapes layer to the viewer\n",
    "shapes_layer = viewer.add_shapes(None, \n",
    "                                 ndim=3,\n",
    "                                 name=label_name, \n",
    "                                 scale=(1,1),\n",
    "                                 edge_width=1,\n",
    "                                 edge_color=current_color,\n",
    "                                 face_color=[1,1,1,0],\n",
    "                                 opacity=.4,\n",
    "                                 )\n",
    "def on_shapes_changed(event):\n",
    "    action = event.action\n",
    "    if action in ['added','removed','changed']:\n",
    "        shape_mask = shapes_layer.to_masks((video_height, video_width))\n",
    "        shape_mask = np.sum(shape_mask, axis=0)\n",
    "        shape_mask[shape_mask > 0] = 1\n",
    "        shape_mask = shape_mask.astype(np.uint8)\n",
    "        frame_idx = viewer.dims.current_step[0] \n",
    "    \n",
    "        label = 1 # Always positive for now\n",
    "        mask = run_new_pred(predictor=predictor,\n",
    "                            frame_idx=frame_idx,\n",
    "                            obj_id=0,\n",
    "                            labels=label,\n",
    "                            masks=shape_mask,\n",
    "                            )\n",
    "\n",
    "        labels_layer.data[frame_idx] = mask\n",
    "        labels_layer.refresh()\n",
    "        \n",
    "        # Prefetch batch of images\n",
    "        if not prefetcher_worker.is_running:\n",
    "            prefetcher_worker.run()\n",
    "    return\n",
    "\n",
    "# Store the initial length of the points data\n",
    "# previous_length_points = len(shapes_layer.data)\n",
    "# Hide the transform, delete, and select buttons\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[shapes_layer]\n",
    "buttons_to_hide = [\n",
    "                   'line_button',\n",
    "                   'move_back_button',\n",
    "                   'move_front_button',\n",
    "                   'path_button',\n",
    "                   'polyline_button',\n",
    "                   ]\n",
    "for btn in buttons_to_hide:\n",
    "    attr = getattr(qctrl, btn)\n",
    "    attr.hide()\n",
    "\n",
    "# Select the current, add tool for the points layer and attach the callback\n",
    "viewer.layers.selection.active = shapes_layer\n",
    "viewer.layers.selection.active.mode = 'pan_zoom'\n",
    "shapes_layer.events.data.connect(on_shapes_changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Points layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the points layer to the viewer\n",
    "points_layer = viewer.add_points(None, \n",
    "                                 ndim=3,\n",
    "                                 name=label_name, \n",
    "                                 scale=(1,1),\n",
    "                                 size=40,\n",
    "                                 border_color='dimgrey',\n",
    "                                 border_width=.2,\n",
    "                                 opacity=.6,\n",
    "                                 )\n",
    "\n",
    "\n",
    "left_right_click = 'left'\n",
    "def on_mouse_press(layer, event):\n",
    "    '''\n",
    "    Generic function to catch left and right mouse clicks\n",
    "    '''\n",
    "    global left_right_click\n",
    "    if event.type == 'mouse_press':\n",
    "        if event.button == 1:  # Left-click\n",
    "            left_right_click = 'left'\n",
    "        elif event.button == 2:  # Right-click\n",
    "            left_right_click = 'right'     \n",
    "    \n",
    "\n",
    "def on_points_changed(event):\n",
    "    '''\n",
    "    Function to run when points are added to the points layer\n",
    "    '''\n",
    "    action = event.action\n",
    "    frame_idx  = viewer.dims.current_step[0] \n",
    "    \n",
    "    left_positive_color  = [0.59607846, 0.98431373, 0.59607846, 1.]\n",
    "    right_negative_color = [1., 1., 1., 1.]\n",
    "    \n",
    "    if action == 'added':\n",
    "        # A new point has just been added. \n",
    "        # Find out if you are dealing with a left or right click    \n",
    "        if left_right_click == 'left':\n",
    "            label = 1\n",
    "            points_layer.face_color[-1] = left_positive_color\n",
    "            points_layer.symbol[-1] = 'o'\n",
    "        elif left_right_click == 'right':\n",
    "            label = 0\n",
    "            points_layer.face_color[-1] = right_negative_color\n",
    "            points_layer.symbol[-1] = 'x'\n",
    "        points_layer.refresh() # THIS IS IMPORTANT\n",
    "        # Prefetch batch of images\n",
    "        if not prefetcher_worker.is_running:\n",
    "            prefetcher_worker.run()\n",
    "        \n",
    "    # Loop through all the data and create points and labels\n",
    "    if action in ['added','removed','changed']:\n",
    "        labels = []\n",
    "        point_data = []\n",
    "        for pt_no, pt in enumerate(points_layer.data):\n",
    "            # Find out which label was attached to the point\n",
    "            # by going through the symbol lists\n",
    "            cur_symbol = points_layer.symbol[pt_no]\n",
    "            if cur_symbol in ['o','disc']:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            labels.append(label)\n",
    "            point_data.append(pt[1:][::-1]) # index 0 is the frame number\n",
    "            \n",
    "        # Then run the actual prediction\n",
    "        mask = run_new_pred(predictor=predictor,\n",
    "                            frame_idx=frame_idx,\n",
    "                            obj_id=0,\n",
    "                            labels=labels,\n",
    "                            points=point_data,\n",
    "                            )\n",
    "        labels_layer.data[frame_idx,:,:] = mask\n",
    "        labels_layer.refresh()   \n",
    "        \n",
    "    \n",
    "points_layer.mouse_drag_callbacks.append(on_mouse_press)\n",
    "points_layer.events.data.connect(on_points_changed)\n",
    "# Select the current, add tool for the points layer\n",
    "viewer.layers.selection.active = points_layer\n",
    "viewer.layers.selection.active.mode = 'add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_id = 0\n",
    "\n",
    "@thread_worker\n",
    "def thread_predict(frame_idx, max_imgs):\n",
    "    global labels_layer\n",
    "\n",
    "    video_segments = {} \n",
    "    start_time = time.time()\n",
    "    # Prefetch images if they are not cached yet \n",
    "    _ = predictor.images[slice(frame_idx,frame_idx+max_imgs)]\n",
    "    \n",
    "    # Loop over frames and run prediction (single frame!)\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(start_frame_idx=frame_idx, \n",
    "                                                                                    max_frame_num_to_track=max_imgs):\n",
    "        \n",
    "        for i, out_obj_id in enumerate(out_obj_ids):\n",
    "            \n",
    "            torch_mask = out_mask_logits[i] > 0.0\n",
    "            # torch_mask = torch_mask.float()\n",
    "            # torch_mask = torch_mask[torch_newaxis,:,:,:]  \n",
    "          \n",
    "            #Perform morphological closing \n",
    "            # torch_mask = kornia_closing(torch_mask, kernel)\n",
    "            out_mask = torch_mask.cpu().numpy()\n",
    "\n",
    "            video_segments[out_frame_idx] = {out_obj_id: out_mask}\n",
    "            if not out_obj_id in predictor.inference_state['centroids']:\n",
    "                predictor.inference_state['centroids'][out_obj_id] = {}\n",
    "            if not out_obj_id in predictor.inference_state['areas']:\n",
    "                predictor.inference_state['areas'][out_obj_id] = {}\n",
    "                \n",
    "        # PICK ONE OBJ (OBJ_ID = 0 or whatever)\n",
    "        \n",
    "        #  Add the mask image as a new labels layer\n",
    "        mask = video_segments[out_frame_idx][obj_id] # THIS NEEDS TO BE MADE LAYER SPECIFIC \n",
    "        current_label = obj_id+1\n",
    "        if len(np.unique(mask))>1:\n",
    "            mask[mask==np.unique(mask)[1]] = current_label \n",
    "\n",
    "        mask = mask.squeeze()\n",
    "        props = measure.regionprops(mask.astype(int))[0]\n",
    "        predictor.inference_state['centroids'][obj_id][out_frame_idx] = props.centroid\n",
    "        predictor.inference_state['areas'][obj_id][out_frame_idx] = props.area\n",
    "        labels_layer.data[out_frame_idx,:,:] = mask\n",
    "        viewer.dims.set_point(0,out_frame_idx)\n",
    "        labels_layer.refresh()\n",
    "    end_time = time.time()\n",
    "    #print(f'start idx {frame_idx} | {max_imgs} frames in {end_time-start_time} s')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import tensor as torch_tensor\n",
    "# from skimage.morphology import disk\n",
    "\n",
    "# predictor.perform_morphological_operations = True\n",
    "\n",
    "# disk_size=10\n",
    "# compute_device=device\n",
    "# predictor.closing_kernel = torch_tensor(disk(disk_size).tolist()).to(compute_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Current chunk size: {chunk_size}')\n",
    "worker = thread_predict(frame_idx=viewer.dims.current_step[0], max_imgs=chunk_size) \n",
    "#worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "worker.start()  # start the thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tried to save the checkpoint, \n",
    "# # but this does not work. \n",
    "# # the check point model_state does not contain enough info \n",
    "# import torch\n",
    "# model_output_path = sample_dir / 'model_output.pth'    \n",
    "# torch.save({\n",
    "#             'model_state_dict': predictor.state_dict(),\n",
    "#             }, model_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Danger zone** Predict the whole video as test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test \n",
    "# for i in range(0,500,chunk_size):\n",
    "    \n",
    "#     prediction_worker = thread_predict(frame_idx=i, max_imgs=chunk_size)  \n",
    "#     prediction_worker.setAutoDelete(True)\n",
    "#     #worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "#     prediction_worker.start()  \n",
    "#     print(f'Highest cached index {int(np.nanmax(predictor.images.cached_indices))}')\n",
    "#     time.sleep(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting\n",
    "# import seaborn as sns\n",
    "# sns.set_theme(style='white')\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "# from matplotlib import pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "# mpl.rcParams.update({\"axes.grid\" : True, \"grid.color\": \"grey\", \"grid.alpha\": .1})\n",
    "# plt.rcParams['xtick.major.size'] = 10\n",
    "# plt.rcParams['xtick.major.width'] = 1\n",
    "# plt.rcParams['ytick.major.size'] = 10\n",
    "# plt.rcParams['ytick.major.width'] = 1\n",
    "# plt.rcParams['xtick.bottom'] = True\n",
    "# plt.rcParams['ytick.left'] = True\n",
    "# mpl.rcParams['savefig.pad_inches'] = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the centroids over time\n",
    "# centroids = list(predictor.inference_state['centroids'][0].values())\n",
    "# centroids = np.stack(centroids)\n",
    "# areas = np.array(list(predictor.inference_state['areas'][0].values())).astype(float)\n",
    "# figure = plt.figure(figsize=(10,10))\n",
    "# plt.imshow(viewer.layers[0].data[0], cmap='gray')\n",
    "# #plt.plot(centroids[:,1], centroids[:,0], '-', color='k', alpha=.6)   \n",
    "# plt.scatter(centroids[:,1], centroids[:,0], s=areas/50, marker='.', color='pink', alpha=.15, lw=0)   \n",
    "# sns.despine(left=True,bottom=True)\n",
    "# plt.title(f'Centroids over time (n={centroids.shape[0]} frames)')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(predictor.inference_state['areas'][0].values()),'-', color='w', alpha=.6 )\n",
    "# plt.title('Area over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict_per_obj is huge \n",
    "# Structure\n",
    "# -> obj_id\n",
    "# --> cond_frame_outputs\n",
    "# --> non_cond_frame_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
