{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Napari tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "cur_path = Path(os.getcwd()).parent\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import cmasher as cmr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='white')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "import zarr\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 10:59:41.248 python[90545:13679475] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-28 10:59:41.248 python[90545:13679475] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the SAM2 model like you do in Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import torch\n",
    "from octron.sam2_octron.helpers.build_sam2_octron import build_sam2_octron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Support for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\n",
      "Uing device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loaded SAM2VideoPredictor OCTRON\n",
      "Model image size: 1024\n"
     ]
    }
   ],
   "source": [
    "sam2_folder = Path('sam2_octron')\n",
    "checkpoint = 'sam2.1_hiera_base_plus.pt' # under folder /checkpoints\n",
    "model_cfg = 'sam2.1/sam2.1_hiera_b+.yaml' # under folder /configs\n",
    "# ------------------------------------------------------------------------------------\n",
    "sam2_checkpoint = cur_path / sam2_folder / Path(f'checkpoints/{checkpoint}')\n",
    "model_cfg = Path(f'configs/{model_cfg}')\n",
    "\n",
    "\n",
    "predictor, device  = build_sam2_octron(config_file=model_cfg.as_posix(), \n",
    "                                                       ckpt_path=sam2_checkpoint.as_posix(), \n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From napari, after loading video file, extract info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 971, 971)\n"
     ]
    }
   ],
   "source": [
    "viewer.dims.set_point(0,0)\n",
    "current_indices = viewer.dims.current_step\n",
    "print(current_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video shape: (1603, 1944, 1944, 3)\n"
     ]
    }
   ],
   "source": [
    "video_data = viewer.layers[0].data   # the whole video\n",
    "\n",
    "print(f'video shape: {video_data.shape}')   \n",
    "num_frames, height, width, channels = video_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_new_pred(inference_state,\n",
    "                frame_idx,\n",
    "                obj_id, \n",
    "                label,\n",
    "                point):\n",
    "    assert label in [0,1]\n",
    "    # Run prediction\n",
    "    #obj_id : give a unique id to each object we interact with (it can be any integers)\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "                                                inference_state=inference_state,\n",
    "                                                frame_idx=frame_idx,\n",
    "                                                obj_id=obj_id,\n",
    "                                                points=np.array([point],dtype=np.float32),\n",
    "                                                labels=np.array([label], np.int32)\n",
    "                                                )\n",
    "    \n",
    "    # Add the mask image as a new labels layer\n",
    "    mask = (out_mask_logits[0] > 0).cpu().numpy().astype(np.uint8)\n",
    "    current_label = obj_id+1\n",
    "    if len(np.unique(mask))>1:\n",
    "        mask[mask==np.unique(mask)[1]] = current_label \n",
    "    mask = mask.squeeze()\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/torch/nn/functional.py:4594: UserWarning: The operator 'aten::upsample_bicubic2d.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  return torch._C._nn.upsample_bicubic2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized SAM2 model\n"
     ]
    }
   ],
   "source": [
    "inference_state = predictor.init_state(napari_viewer=viewer, video_layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/t95cmymn5cj5mmyw65m26t4w0000gn/T/ipykernel_90545/1617221865.py:89: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  qctrl = viewer.window.qt_viewer.controls.widgets[points_layer]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left click at [  49.          570.05264464 1557.68424759]\n",
      "New point added on frame 49 at [1557.68424759  570.05264464]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/app/backends/_qt.py\", line 506, in mouseReleaseEvent\n",
      "    self._vispy_mouse_release(\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/app/base.py\", line 224, in _vispy_mouse_release\n",
      "    ev = self._vispy_canvas.events.mouse_release(**kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/util/event.py\", line 453, in __call__\n",
      "    self._invoke_callback(cb, event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/util/event.py\", line 471, in _invoke_callback\n",
      "    _handle_exception(self.ignore_callback_errors,\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/util/event.py\", line 469, in _invoke_callback\n",
      "    cb(event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/_vispy/canvas.py\", line 484, in _on_mouse_release\n",
      "    self._process_mouse_event(mouse_release_callbacks, event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/_vispy/canvas.py\", line 417, in _process_mouse_event\n",
      "    mouse_callbacks(layer, event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/interactions.py\", line 216, in mouse_release_callbacks\n",
      "    next(gen)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/layers/points/_points_mouse_bindings.py\", line 141, in add\n",
      "    layer.add(coordinates)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/layers/points/points.py\", line 2119, in add\n",
      "    self.events.data(\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/events/event.py\", line 764, in __call__\n",
      "    self._invoke_callback(cb, event if pass_event else None)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/events/event.py\", line 802, in _invoke_callback\n",
      "    _handle_exception(\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/events/event.py\", line 789, in _invoke_callback\n",
      "    cb(event)\n",
      "  File \"/var/folders/01/t95cmymn5cj5mmyw65m26t4w0000gn/T/ipykernel_90545/1617221865.py\", line 75, in on_points_added\n",
      "    mask = run_new_pred(inference_state,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/01/t95cmymn5cj5mmyw65m26t4w0000gn/T/ipykernel_90545/2156708667.py\", line 9, in run_new_pred\n",
      "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/Documents/python/segment-anything-2/sam2/sam2_video_predictor.py\", line 263, in add_new_points_or_box\n",
      "    current_out, _ = self._run_single_frame_inference(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/Documents/python/OCTRON/octron/sam2_octron/helpers/sam2_octron.py\", line 160, in _run_single_frame_inference\n",
      "    ) = self._get_image_feature(inference_state, frame_idx, batch_size)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/Documents/python/segment-anything-2/sam2/sam2_video_predictor.py\", line 713, in _get_image_feature\n",
      "    image = inference_state[\"images\"][frame_idx].to(device).float().unsqueeze(0)\n",
      "            ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "IndexError: index 49 is out of bounds for dimension 0 with size 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right click at [  49.          570.05264464 1557.68424759]\n",
      "New point added on frame 49 at [1557.68424759  570.05264464]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/app/backends/_qt.py\", line 506, in mouseReleaseEvent\n",
      "    self._vispy_mouse_release(\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/app/base.py\", line 224, in _vispy_mouse_release\n",
      "    ev = self._vispy_canvas.events.mouse_release(**kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/util/event.py\", line 453, in __call__\n",
      "    self._invoke_callback(cb, event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/util/event.py\", line 471, in _invoke_callback\n",
      "    _handle_exception(self.ignore_callback_errors,\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/util/event.py\", line 469, in _invoke_callback\n",
      "    cb(event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/_vispy/canvas.py\", line 484, in _on_mouse_release\n",
      "    self._process_mouse_event(mouse_release_callbacks, event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/_vispy/canvas.py\", line 417, in _process_mouse_event\n",
      "    mouse_callbacks(layer, event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/interactions.py\", line 216, in mouse_release_callbacks\n",
      "    next(gen)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/layers/points/_points_mouse_bindings.py\", line 141, in add\n",
      "    layer.add(coordinates)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/layers/points/points.py\", line 2119, in add\n",
      "    self.events.data(\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/events/event.py\", line 764, in __call__\n",
      "    self._invoke_callback(cb, event if pass_event else None)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/events/event.py\", line 802, in _invoke_callback\n",
      "    _handle_exception(\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/events/event.py\", line 789, in _invoke_callback\n",
      "    cb(event)\n",
      "  File \"/var/folders/01/t95cmymn5cj5mmyw65m26t4w0000gn/T/ipykernel_90545/1617221865.py\", line 75, in on_points_added\n",
      "    mask = run_new_pred(inference_state,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/01/t95cmymn5cj5mmyw65m26t4w0000gn/T/ipykernel_90545/2156708667.py\", line 9, in run_new_pred\n",
      "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/Documents/python/segment-anything-2/sam2/sam2_video_predictor.py\", line 263, in add_new_points_or_box\n",
      "    current_out, _ = self._run_single_frame_inference(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/Documents/python/OCTRON/octron/sam2_octron/helpers/sam2_octron.py\", line 160, in _run_single_frame_inference\n",
      "    ) = self._get_image_feature(inference_state, frame_idx, batch_size)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/Documents/python/segment-anything-2/sam2/sam2_video_predictor.py\", line 713, in _get_image_feature\n",
      "    image = inference_state[\"images\"][frame_idx].to(device).float().unsqueeze(0)\n",
      "            ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "IndexError: index 49 is out of bounds for dimension 0 with size 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the mask and annotation layers \n",
    "# Keep them empty at start \n",
    "mask_layer_dummy = np.zeros((num_frames, height, width), dtype=np.uint8)\n",
    "mask_layer_dummy.shape\n",
    "\n",
    "colors = cmr.take_cmap_colors('cmr.tropical', 8, cmap_range=(0, 1),\n",
    "                     return_fmt='int')\n",
    "colors_norm = np.stack(colors) / 255.0  \n",
    "cyclic_map = napari.utils.CyclicLabelColormap(\n",
    "    np.hstack([np.stack(colors_norm), np.ones((len(colors), 1))])\n",
    ")\n",
    "labels_layer = viewer.add_labels(\n",
    "    mask_layer_dummy, \n",
    "    name='Mask',  # Name of the layer\n",
    "    opacity=0.4,  # Optional: opacity of the labels\n",
    "    colormap=cyclic_map,\n",
    "    blending='additive'  # Optional: blending mode\n",
    ")\n",
    "# Add the points layer to the viewer\n",
    "points_layer = viewer.add_points(None, \n",
    "                                 ndim=3,\n",
    "                                 name='Annotations', \n",
    "                                 scale=(1,1),\n",
    "                                 size=40,\n",
    "                                 border_color='dimgrey',\n",
    "                                 border_width=.2,\n",
    "                                 opacity=.6,\n",
    "                                 )\n",
    "# Store the initial length of the points data\n",
    "previous_length = len(points_layer.data)\n",
    "\n",
    "\n",
    "left_right_click = 'left'\n",
    "def on_mouse_press(layer, event):\n",
    "    '''\n",
    "    Generic function to catch left and right mouse clicks\n",
    "    '''\n",
    "    global left_right_click\n",
    "    if event.type == 'mouse_press':\n",
    "        if event.button == 1:  # Left-click\n",
    "            left_right_click = 'left'\n",
    "        elif event.button == 2:  # Right-click\n",
    "            left_right_click = 'right'\n",
    "\n",
    "\n",
    "def on_points_added(event):\n",
    "\n",
    "    global points_layer\n",
    "    global labels_layer\n",
    "    global left_right_click\n",
    "    \n",
    "    global previous_length\n",
    "    global inference_state\n",
    "    \n",
    "    current_length = len(points_layer.data)\n",
    "    if current_length > previous_length:\n",
    "        previous_length = current_length \n",
    "\n",
    "        # # Execute prediction \n",
    "        newest_point_data =  points_layer.data[-1]\n",
    "        print(f'{left_right_click} click at {newest_point_data}')\n",
    "        if left_right_click == 'left':\n",
    "            label = 1\n",
    "            points_layer.face_color[-1] = [0.59607846, 0.98431373, 0.59607846, 1.]\n",
    "            points_layer.symbol[-1] = 'o'\n",
    "        elif left_right_click == 'right':\n",
    "            label = 0\n",
    "            points_layer.face_color[-1] = [1., 1., 1., 1.]\n",
    "            points_layer.symbol[-1] = 'x'\n",
    "        points_layer.refresh() \n",
    "        # Run prediction\n",
    "        frame_idx  = int(newest_point_data[0])\n",
    "        point_data = newest_point_data[1:][::-1]\n",
    "        print(f\"New point added on frame {frame_idx} at {point_data}\")  \n",
    "        mask = run_new_pred(inference_state,\n",
    "                            frame_idx=frame_idx,\n",
    "                            obj_id=0,\n",
    "                            label=label,\n",
    "                            point=point_data,\n",
    "                            )\n",
    "        \n",
    "        labels_layer.data[frame_idx,:,:]= mask\n",
    "        labels_layer.refresh()   \n",
    "\n",
    "points_layer.mouse_drag_callbacks.append(on_mouse_press)\n",
    "points_layer.events.data.connect(on_points_added)\n",
    "\n",
    "# Hide the transform, delete, and select buttons\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[points_layer]\n",
    "getattr(qctrl, 'transform_button').setVisible(False)\n",
    "getattr(qctrl, 'delete_button').setVisible(False)\n",
    "getattr(qctrl, 'select_button').setVisible(False)\n",
    "# Select the current, add tool for the points layer\n",
    "viewer.layers.selection.active = points_layer\n",
    "viewer.layers.selection.active.mode = 'add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.remove_object(inference_state, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from napari.qt.threading import thread_worker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_id = 0\n",
    "\n",
    "@thread_worker\n",
    "def thread_predict(inference_state, frame_idx, max_imgs):\n",
    "    global labels_layer\n",
    "    video_segments = {} \n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state, 0, max_imgs):\n",
    "        video_segments[out_frame_idx] = {\n",
    "                                            out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "                                            for i, out_obj_id in enumerate(out_obj_ids)\n",
    "                                        }\n",
    "        #  Add the mask image as a new labels layer\n",
    "        mask = video_segments[out_frame_idx][obj_id] # THIS NEEDS TO BE MADE LAYER SPECIFIC \n",
    "        current_label = obj_id+1\n",
    "        if len(np.unique(mask))>1:\n",
    "            mask[mask==np.unique(mask)[1]] = current_label \n",
    "        mask = mask.squeeze()\n",
    "        labels_layer.data[out_frame_idx,:,:]= mask\n",
    "        labels_layer.refresh()   \n",
    "        viewer.dims.set_point(0,out_frame_idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker = thread_predict(inference_state, frame_idx=0 , max_imgs=200)  # create \"worker\" object\n",
    "#worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "worker.start()  # start the thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zarr experiments \n",
    "\n",
    "start_idx = 0   \n",
    "stop_idx = 200\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "# Create temp output dir \n",
    "sample_dir = cur_path / 'sample_data'\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "sample_data_zarr = sample_dir / 'sample_data.zarr'\n",
    "if sample_data_zarr.exists():\n",
    "    shutil.rmtree(sample_data_zarr)\n",
    "\n",
    "store = zarr.storage.LocalStore(sample_data_zarr, read_only=False)\n",
    "root = zarr.create_group(store=store)\n",
    "\n",
    "# # Create a subgroup for the current object ID\n",
    "# annotation_group = root.create_group(ann_name)\n",
    "\n",
    "\n",
    "image_zarr = root.create_array(name='masks',\n",
    "                                shape=(num_frames, 3, predictor.image_size, predictor.image_size),  \n",
    "                                chunks=(batch_size, 3, predictor.image_size, predictor.image_size),  \n",
    "                                dtype='float16')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 0 to 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 20 to 40\n",
      "Processing frames 40 to 60\n",
      "Processing frames 60 to 80\n",
      "Processing frames 80 to 100\n",
      "Processing frames 100 to 120\n",
      "Processing frames 120 to 140\n",
      "Processing frames 140 to 160\n",
      "Processing frames 160 to 180\n",
      "Processing frames 180 to 200\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Resize\n",
    "\n",
    "@thread_worker\n",
    "def resize_to_zarr(image_zarr, start_idx, stop_idx, batch_size):   \n",
    "    resize_img = Resize(\n",
    "                    size=(predictor.image_size), # This is 1024x1024 for the l model\n",
    "                    )\n",
    "    for idx in range(start_idx, stop_idx, batch_size):\n",
    "        print(f'Processing frames {idx} to {idx+batch_size}')\n",
    "        collected_imgs = []\n",
    "        for img_idx in range(idx, idx+batch_size):   \n",
    "            curr_img = torch.from_numpy(video_data[img_idx]).permute(2, 0, 1)\n",
    "            curr_img = resize_img(curr_img)\n",
    "            curr_img = curr_img.float() / 255.0\n",
    "            collected_imgs.append(curr_img)\n",
    "        image_zarr[idx:idx+batch_size] = torch.stack(collected_imgs).numpy()\n",
    "        \n",
    "worker = resize_to_zarr(image_zarr, start_idx, stop_idx, batch_size)  \n",
    "worker.start()  # start the thread!\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in image_zarr[:]:\n",
    "    image = torch.from_numpy(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024, 1024])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left click at [  19.          516.76316966 1433.34213931]\n",
      "New point added on frame 19 at [1433.34213931  516.76316966]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/app/backends/_qt.py\", line 506, in mouseReleaseEvent\n",
      "    self._vispy_mouse_release(\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/app/base.py\", line 224, in _vispy_mouse_release\n",
      "    ev = self._vispy_canvas.events.mouse_release(**kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/util/event.py\", line 453, in __call__\n",
      "    self._invoke_callback(cb, event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/util/event.py\", line 471, in _invoke_callback\n",
      "    _handle_exception(self.ignore_callback_errors,\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/vispy/util/event.py\", line 469, in _invoke_callback\n",
      "    cb(event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/_vispy/canvas.py\", line 484, in _on_mouse_release\n",
      "    self._process_mouse_event(mouse_release_callbacks, event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/_vispy/canvas.py\", line 417, in _process_mouse_event\n",
      "    mouse_callbacks(layer, event)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/interactions.py\", line 216, in mouse_release_callbacks\n",
      "    next(gen)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/layers/points/_points_mouse_bindings.py\", line 141, in add\n",
      "    layer.add(coordinates)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/layers/points/points.py\", line 2119, in add\n",
      "    self.events.data(\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/events/event.py\", line 764, in __call__\n",
      "    self._invoke_callback(cb, event if pass_event else None)\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/events/event.py\", line 802, in _invoke_callback\n",
      "    _handle_exception(\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/napari/utils/events/event.py\", line 789, in _invoke_callback\n",
      "    cb(event)\n",
      "  File \"/var/folders/01/t95cmymn5cj5mmyw65m26t4w0000gn/T/ipykernel_90154/1617221865.py\", line 75, in on_points_added\n",
      "    mask = run_new_pred(inference_state,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/01/t95cmymn5cj5mmyw65m26t4w0000gn/T/ipykernel_90154/2156708667.py\", line 9, in run_new_pred\n",
      "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/Documents/python/segment-anything-2/sam2/sam2_video_predictor.py\", line 263, in add_new_points_or_box\n",
      "    current_out, _ = self._run_single_frame_inference(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/Documents/python/segment-anything-2/sam2/sam2_video_predictor.py\", line 758, in _run_single_frame_inference\n",
      "    ) = self._get_image_feature(inference_state, frame_idx, batch_size)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/horst/Documents/python/segment-anything-2/sam2/sam2_video_predictor.py\", line 713, in _get_image_feature\n",
      "    image = inference_state[\"images\"][frame_idx].to(device).float().unsqueeze(0)\n",
      "            ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "IndexError: index 19 is out of bounds for dimension 0 with size 1\n"
     ]
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  inference_state[\"output_dict_per_obj\"][obj_idx] = {\n",
    "#                 \"cond_frame_outputs\": {},  # dict containing {frame_idx: <out>}\n",
    "#                 \"non_cond_frame_outputs\": {},  # dict containing {frame_idx: <out>}\n",
    "#             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict_per_obj is huge \n",
    "# Structure\n",
    "# -> obj_id\n",
    "# --> cond_frame_outputs\n",
    "# --> non_cond_frame_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state[\"output_dict_per_obj\"][obj_idx]['cond_frame_outputs'][0]['obj_ptr'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
