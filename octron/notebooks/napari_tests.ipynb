{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Napari tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "cur_path = Path(os.getcwd()).parent\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import cmasher as cmr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='white')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "from napari.experimental import link_layers\n",
    "import zarr\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 17:51:02.779 python[78663:13105539] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-27 17:51:02.779 python[78663:13105539] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the SAM2 model like you do in Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import torch\n",
    "from octron.sam2_octron.helpers.build_sam_octron import build_sam2_video_predictor_octron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Support for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\n",
      "Uing device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loaded SAM2VideoPredictor OCTRON\n",
      "Model image size: 1024\n"
     ]
    }
   ],
   "source": [
    "sam2_folder = Path('sam2_octron')\n",
    "checkpoint = 'sam2.1_hiera_base_plus.pt' # under folder /checkpoints\n",
    "model_cfg = 'sam2.1/sam2.1_hiera_b+.yaml' # under folder /configs\n",
    "# ------------------------------------------------------------------------------------\n",
    "sam2_checkpoint = cur_path / sam2_folder / Path(f'checkpoints/{checkpoint}')\n",
    "model_cfg = Path(f'configs/{model_cfg}')\n",
    "\n",
    "\n",
    "predictor, device  = build_sam2_video_predictor_octron(config_file=model_cfg.as_posix(), \n",
    "                                                       ckpt_path=sam2_checkpoint.as_posix(), \n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From napari, after loading video file, extract info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 499, 499)\n"
     ]
    }
   ],
   "source": [
    "viewer.dims.set_point(0,0)\n",
    "current_indices = viewer.dims.current_step\n",
    "print(current_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video shape: (4067, 1000, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "video_data = viewer.layers[0].data   # the whole video\n",
    "\n",
    "print(f'video shape: {video_data.shape}')   \n",
    "num_frames, height, width, channels = video_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_new_pred(inference_state,\n",
    "                frame_idx,\n",
    "                obj_id, \n",
    "                label,\n",
    "                point):\n",
    "    assert label in [0,1]\n",
    "    # Run prediction\n",
    "    #obj_id : give a unique id to each object we interact with (it can be any integers)\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "                                                inference_state=inference_state,\n",
    "                                                frame_idx=frame_idx,\n",
    "                                                obj_id=obj_id,\n",
    "                                                points=np.array([point],dtype=np.float32),\n",
    "                                                labels=np.array([label], np.int32)\n",
    "                                                )\n",
    "    \n",
    "    # Add the mask image as a new labels layer\n",
    "    mask = (out_mask_logits[0] > 0).cpu().numpy().astype(np.uint8)\n",
    "    current_label = obj_id+1\n",
    "    if len(np.unique(mask))>1:\n",
    "        mask[mask==np.unique(mask)[1]] = current_label \n",
    "    mask = mask.squeeze()\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/torch/nn/functional.py:4594: UserWarning: The operator 'aten::upsample_bicubic2d.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  return torch._C._nn.upsample_bicubic2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized SAM2 model\n"
     ]
    }
   ],
   "source": [
    "inference_state = predictor.init_state(napari_viewer=viewer, video_layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/t95cmymn5cj5mmyw65m26t4w0000gn/T/ipykernel_78663/1617221865.py:89: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  qctrl = viewer.window.qt_viewer.controls.widgets[points_layer]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the mask and annotation layers \n",
    "# Keep them empty at start \n",
    "mask_layer_dummy = np.zeros((num_frames, height, width), dtype=np.uint8)\n",
    "mask_layer_dummy.shape\n",
    "\n",
    "colors = cmr.take_cmap_colors('cmr.tropical', 8, cmap_range=(0, 1),\n",
    "                     return_fmt='int')\n",
    "colors_norm = np.stack(colors) / 255.0  \n",
    "cyclic_map = napari.utils.CyclicLabelColormap(\n",
    "    np.hstack([np.stack(colors_norm), np.ones((len(colors), 1))])\n",
    ")\n",
    "labels_layer = viewer.add_labels(\n",
    "    mask_layer_dummy, \n",
    "    name='Mask',  # Name of the layer\n",
    "    opacity=0.4,  # Optional: opacity of the labels\n",
    "    colormap=cyclic_map,\n",
    "    blending='additive'  # Optional: blending mode\n",
    ")\n",
    "# Add the points layer to the viewer\n",
    "points_layer = viewer.add_points(None, \n",
    "                                 ndim=3,\n",
    "                                 name='Annotations', \n",
    "                                 scale=(1,1),\n",
    "                                 size=40,\n",
    "                                 border_color='dimgrey',\n",
    "                                 border_width=.2,\n",
    "                                 opacity=.6,\n",
    "                                 )\n",
    "# Store the initial length of the points data\n",
    "previous_length = len(points_layer.data)\n",
    "\n",
    "\n",
    "left_right_click = 'left'\n",
    "def on_mouse_press(layer, event):\n",
    "    '''\n",
    "    Generic function to catch left and right mouse clicks\n",
    "    '''\n",
    "    global left_right_click\n",
    "    if event.type == 'mouse_press':\n",
    "        if event.button == 1:  # Left-click\n",
    "            left_right_click = 'left'\n",
    "        elif event.button == 2:  # Right-click\n",
    "            left_right_click = 'right'\n",
    "\n",
    "\n",
    "def on_points_added(event):\n",
    "\n",
    "    global points_layer\n",
    "    global labels_layer\n",
    "    global left_right_click\n",
    "    \n",
    "    global previous_length\n",
    "    global inference_state\n",
    "    \n",
    "    current_length = len(points_layer.data)\n",
    "    if current_length > previous_length:\n",
    "        previous_length = current_length \n",
    "\n",
    "        # # Execute prediction \n",
    "        newest_point_data =  points_layer.data[-1]\n",
    "        print(f'{left_right_click} click at {newest_point_data}')\n",
    "        if left_right_click == 'left':\n",
    "            label = 1\n",
    "            points_layer.face_color[-1] = [0.59607846, 0.98431373, 0.59607846, 1.]\n",
    "            points_layer.symbol[-1] = 'o'\n",
    "        elif left_right_click == 'right':\n",
    "            label = 0\n",
    "            points_layer.face_color[-1] = [1., 1., 1., 1.]\n",
    "            points_layer.symbol[-1] = 'x'\n",
    "        points_layer.refresh() \n",
    "        # Run prediction\n",
    "        frame_idx  = int(newest_point_data[0])\n",
    "        point_data = newest_point_data[1:][::-1]\n",
    "        print(f\"New point added on frame {frame_idx} at {point_data}\")  \n",
    "        mask = run_new_pred(inference_state,\n",
    "                            frame_idx=frame_idx,\n",
    "                            obj_id=0,\n",
    "                            label=label,\n",
    "                            point=point_data,\n",
    "                            )\n",
    "        \n",
    "        labels_layer.data[frame_idx,:,:]= mask\n",
    "        labels_layer.refresh()   \n",
    "\n",
    "points_layer.mouse_drag_callbacks.append(on_mouse_press)\n",
    "points_layer.events.data.connect(on_points_added)\n",
    "\n",
    "# Hide the transform, delete, and select buttons\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[points_layer]\n",
    "getattr(qctrl, 'transform_button').setVisible(False)\n",
    "getattr(qctrl, 'delete_button').setVisible(False)\n",
    "getattr(qctrl, 'select_button').setVisible(False)\n",
    "# Select the current, add tool for the points layer\n",
    "viewer.layers.selection.active = points_layer\n",
    "viewer.layers.selection.active.mode = 'add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left click at [  0.         539.70465421 241.82455212]\n",
      "New point added on frame 0 at [241.82455212 539.70465421]\n",
      "left click at [  0.         543.3596248  232.68712563]\n",
      "New point added on frame 0 at [232.68712563 543.3596248 ]\n"
     ]
    }
   ],
   "source": [
    "predictor.remove_object(inference_state, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from napari.qt.threading import thread_worker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_id = 0\n",
    "\n",
    "@thread_worker\n",
    "def thread_predict(inference_state, frame_idx, max_imgs):\n",
    "    global labels_layer\n",
    "    video_segments = {} \n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state, 0, max_imgs):\n",
    "        video_segments[out_frame_idx] = {\n",
    "                                            out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "                                            for i, out_obj_id in enumerate(out_obj_ids)\n",
    "                                        }\n",
    "        #  Add the mask image as a new labels layer\n",
    "        mask = video_segments[out_frame_idx][obj_id] # THIS NEEDS TO BE MADE LAYER SPECIFIC \n",
    "        current_label = obj_id+1\n",
    "        if len(np.unique(mask))>1:\n",
    "            mask[mask==np.unique(mask)[1]] = current_label \n",
    "        mask = mask.squeeze()\n",
    "        labels_layer.data[out_frame_idx,:,:]= mask\n",
    "        labels_layer.refresh()   \n",
    "        viewer.dims.set_point(0,out_frame_idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 201/201 [01:14<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "worker = thread_predict(inference_state, frame_idx=0 , max_imgs=200)  # create \"worker\" object\n",
    "#worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "worker.start()  # start the thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  inference_state[\"output_dict_per_obj\"][obj_idx] = {\n",
    "#                 \"cond_frame_outputs\": {},  # dict containing {frame_idx: <out>}\n",
    "#                 \"non_cond_frame_outputs\": {},  # dict containing {frame_idx: <out>}\n",
    "#             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict_per_obj is huge \n",
    "# Structure\n",
    "# -> obj_id\n",
    "# --> cond_frame_outputs\n",
    "# --> non_cond_frame_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state[\"output_dict_per_obj\"][obj_idx]['cond_frame_outputs'][0]['obj_ptr'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
