{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Napari tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "cur_path = Path(os.getcwd()).parent\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import cmasher as cmr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='white')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "from napari.qt.threading import thread_worker\n",
    "from napari.utils import DirectLabelColormap\n",
    "import warnings\n",
    "warnings.simplefilter(action='always', category=FutureWarning)\n",
    "import zarr\n",
    "import time\n",
    "\n",
    "#### Importing additional stuff \n",
    "from skimage import measure\n",
    "from skimage.draw import polygon2mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the SAM2 model like you do in Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "from octron.sam2_octron.helpers.build_sam2_octron import build_sam2_octron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Support for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\n",
      "Uing device: mps\n",
      "\n",
      "\n",
      "Loaded SAM2VideoPredictor OCTRON\n",
      "Model image size: 1024\n"
     ]
    }
   ],
   "source": [
    "sam2_folder = Path('sam2_octron')\n",
    "checkpoint = 'sam2.1_hiera_base_plus.pt' # under folder /checkpoints\n",
    "model_cfg = 'sam2.1/sam2.1_hiera_b+.yaml' # under folder /configs\n",
    "# ----------------------------------------------------------------------------\n",
    "sam2_checkpoint = cur_path / sam2_folder / Path(f'checkpoints/{checkpoint}')\n",
    "model_cfg = Path(f'configs/{model_cfg}')\n",
    "\n",
    "\n",
    "predictor, device  = build_sam2_octron(config_file=model_cfg.as_posix(), \n",
    "                                       ckpt_path=sam2_checkpoint.as_posix(), \n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From napari, after loading video file, extract info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 499, 499)\n"
     ]
    }
   ],
   "source": [
    "viewer.dims.set_point(0,0)\n",
    "current_indices = viewer.dims.current_step\n",
    "print(current_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video shape: (4067, 1000, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "video_data = viewer.layers[0].data   # the whole video\n",
    "\n",
    "print(f'video shape: {video_data.shape}')   \n",
    "num_frames, height, width, channels = video_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zarr store to save all resized images \n",
    "chunk_size = 20\n",
    "\n",
    "\n",
    "\n",
    "# Create temp output dir \n",
    "sample_dir = cur_path / 'sample_data'\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "sample_data_zarr = sample_dir / 'sample_data.zip'\n",
    "if sample_data_zarr.exists():\n",
    "    os.remove(sample_data_zarr)\n",
    "\n",
    "# Assuming local store on fast SSD, so no compression employed for now \n",
    "store = zarr.storage.ZipStore(sample_data_zarr, mode='w')\n",
    "root = zarr.create_group(store=store)\n",
    "image_zarr = root.create_array(name='masks',\n",
    "                                shape=(num_frames, 3, predictor.image_size, predictor.image_size),  \n",
    "                                chunks=(chunk_size, 3, predictor.image_size, predictor.image_size), \n",
    "                                fill_value=np.nan,\n",
    "                                dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/horst/miniconda3/envs/sam2/lib/python3.11/site-packages/torch/nn/functional.py:4594: UserWarning: The operator 'aten::upsample_bicubic2d.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  return torch._C._nn.upsample_bicubic2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized SAM2 model\n"
     ]
    }
   ],
   "source": [
    "predictor.init_state(napari_viewer=viewer, video_layer_idx=0, zarr_store=image_zarr)\n",
    "predictor.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to load previous checkpoint does not work \n",
    "# Since the model still expects at least one input\n",
    "\n",
    "# import torch \n",
    "# state_path = '/Users/horst/Documents/python/OCTRON/octron/sample_data/model_output.pth'\n",
    "# checkpoint = torch.load(state_path, weights_only=True)\n",
    "# predictor.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_new_pred(frame_idx,\n",
    "                 obj_id, \n",
    "                 label,\n",
    "                 point=None,\n",
    "                 mask=None\n",
    "                 ):\n",
    "    assert label in [0,1], f'label must be 0 or 1, got {label}'\n",
    "    assert point is not None or mask is not None\n",
    "    if mask is not None:\n",
    "        assert len(mask.shape) == 2\n",
    "        \n",
    "        print('Running mask prediction')\n",
    "        frame_idx, obj_ids, video_res_masks = predictor.add_new_mask(\n",
    "                                                    frame_idx=frame_idx,\n",
    "                                                    obj_id=obj_id,\n",
    "                                                    mask=np.array(mask, dtype=bool)\n",
    "                                                    )\n",
    "        mask = (video_res_masks[0] > 0).cpu().numpy().astype(np.uint8)\n",
    "                \n",
    "        \n",
    "    if point is not None:\n",
    "        assert len(point) == 2\n",
    "        # Run point prediction\n",
    "        print('Running point prediction')\n",
    "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "                                                    frame_idx=frame_idx,\n",
    "                                                    obj_id=obj_id,\n",
    "                                                    points=np.array([point],dtype=np.float32),\n",
    "                                                    labels=np.array([label], np.int32)\n",
    "                                                    )\n",
    "        \n",
    "        # Add the mask image as a new labels layer\n",
    "        mask = (out_mask_logits[0] > 0).cpu().numpy().astype(np.uint8)\n",
    "        \n",
    "    current_label = obj_id+1\n",
    "    if len(np.unique(mask))>1:\n",
    "        mask[mask==np.unique(mask)[1]] = current_label \n",
    "    mask = mask.squeeze()\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prefetcher\n"
     ]
    }
   ],
   "source": [
    "# Set up thread worker to deal with prefetching batches of images\n",
    "@thread_worker\n",
    "def thread_prefetch_images(batch_size):\n",
    "    print('Running prefetcher')\n",
    "    global viewer\n",
    "    current_indices = viewer.dims.current_step\n",
    "    _ = predictor.images[slice(current_indices[0],current_indices[0]+batch_size)]\n",
    "prefetcher_worker = thread_prefetch_images(chunk_size)   \n",
    "prefetcher_worker.setAutoDelete(False)\n",
    "prefetcher_worker.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - create a continous colormap and cmap_range for each label \n",
    "# --> this way we get sub colormaps and for the same label name can cycle through these \n",
    "def create_label_colors(cmap='cmr.tropical', label_n=4, obj_n=5):\n",
    "    '''\n",
    "    Create color map dictionary for labels \n",
    "    label(int) -> color list -> color(4D)\n",
    "    \n",
    "    For each label (n=label_cat_n) create a sub colormap with color_cat_n colors.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    label_cat_rel = np.linspace(0,1,label_n+1) # This must be the ugliest written fctn in the world\n",
    "\n",
    "    label_colors = {}\n",
    "    for cat in range(len(label_cat_rel)-1):\n",
    "        rel_range = label_cat_rel[cat:cat+2]\n",
    "        colors = np.array(cmr.take_cmap_colors(cmap, \n",
    "                                              obj_n, \n",
    "                                              cmap_range=(rel_range[0], rel_range[1]), \n",
    "                                              return_fmt='int'\n",
    "                                              ) \n",
    "                        ) / 255.0\n",
    "        colors4d = np.hstack([colors, np.ones((len(colors), 1))])\n",
    "        label_colors[cat] = {i+1: color for i, color in enumerate(colors4d)} # Keys start at 1 !\n",
    "        label_colors[cat][None] = np.array([0,0,0,0]).astype(np.float32)\n",
    "    return label_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 1 # Only plays a role here ... not in predictor  \n",
    "obj_id = 0\n",
    "\n",
    "colors = create_label_colors(cmap='cmr.tropical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "/var/folders/01/t95cmymn5cj5mmyw65m26t4w0000gn/T/ipykernel_19086/3974842889.py:20: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  qctrl = viewer.window.qt_viewer.controls.widgets[labels_layer]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the mask and annotation layers \n",
    "# Keep them empty at start \n",
    "mask_layer_dummy = np.zeros((num_frames, height, width), dtype=np.uint8)\n",
    "mask_layer_dummy.shape\n",
    "\n",
    "\n",
    "# Select colormap for labels layer based on category (label) and current object ID \n",
    "current_color_labelmap = DirectLabelColormap(color_dict=colors[label_id], \n",
    "                                             use_selection=True, \n",
    "                                             selection=obj_id+1,\n",
    "                                             )\n",
    "labels_layer = viewer.add_labels(\n",
    "    mask_layer_dummy, \n",
    "    name='Mask',  # Name of the layer\n",
    "    opacity=0.4,  # Optional: opacity of the labels\n",
    "    blending='additive',  # Optional: blending mode\n",
    "    colormap=current_color_labelmap, \n",
    ")\n",
    "\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[labels_layer]\n",
    "buttons_to_hide = ['erase_button',\n",
    "                   'fill_button',\n",
    "                   'paint_button',\n",
    "                   'pick_button',\n",
    "                   'polygon_button',\n",
    "                   'transform_button',\n",
    "                   ]\n",
    "for btn in buttons_to_hide:\n",
    "    getattr(qctrl, btn).setEnabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/t95cmymn5cj5mmyw65m26t4w0000gn/T/ipykernel_19086/1588443298.py:56: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  qctrl = viewer.window.qt_viewer.controls.widgets[shapes_layer]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.on_shapes_added(event)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the shapes layer to the viewer\n",
    "shapes_layer = viewer.add_shapes(None, \n",
    "                                 ndim=3,\n",
    "                                 name='Shape annotations', \n",
    "                                 scale=(1,1),\n",
    "                                 edge_width=0,\n",
    "                                 face_color=colors[label_id][obj_id+1],\n",
    "                                 opacity=.5,\n",
    "                                 )\n",
    "\n",
    "# Store the initial length of the shapes data\n",
    "previous_length_shapes = len(shapes_layer.data)\n",
    "\n",
    "# Function to convert polygon data to mask\n",
    "def polygons_to_mask(polygon, shape):\n",
    "    mask = np.zeros(shape, dtype=np.uint8)\n",
    "    mask = polygon2mask(mask.shape, polygon)\n",
    "    mask = mask.astype(np.uint8)\n",
    "    return mask\n",
    "\n",
    "def on_shapes_added(event):\n",
    "    global shapes_layer\n",
    "    global labels_layer\n",
    "    global previous_length_shapes\n",
    "    global height, width\n",
    "    current_length = len(shapes_layer.data)\n",
    "    if current_length > previous_length_shapes:\n",
    "        previous_length_shapes = current_length \n",
    "\n",
    "        # Execute prediction \n",
    "        newest_shape_data =  shapes_layer.data[-1]    \n",
    "        frame_idx = int(newest_shape_data[:,0][0])\n",
    "        print(frame_idx)\n",
    "        input_mask = polygons_to_mask(newest_shape_data[:,1:], (height, width))\n",
    "        label = 1\n",
    "        mask = run_new_pred(frame_idx=frame_idx,\n",
    "                            obj_id=0,\n",
    "                            label=label,\n",
    "                            mask=input_mask,\n",
    "                            )\n",
    "        \n",
    "        labels_layer.data[frame_idx] = mask\n",
    "        labels_layer.refresh()\n",
    "        \n",
    "        # Prefetch batch of images\n",
    "        # This is done here since adding it as direct mouse interaction \n",
    "        # slows down the first prediction\n",
    "        if not prefetcher_worker.is_running:\n",
    "            prefetcher_worker.run()\n",
    "        \n",
    "    return\n",
    "\n",
    "# Store the initial length of the points data\n",
    "# previous_length_points = len(shapes_layer.data)\n",
    "# Hide the transform, delete, and select buttons\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[shapes_layer]\n",
    "buttons_to_hide = ['transform_button', \n",
    "                   'delete_button', \n",
    "                   'select_button', \n",
    "                   'direct_button',\n",
    "                   'ellipse_button',\n",
    "                   'line_button',\n",
    "                   'move_back_button',\n",
    "                   'move_front_button',\n",
    "                   'path_button',\n",
    "                   'polygon_button',\n",
    "                   'polyline_button',\n",
    "                   'rectangle_button',\n",
    "                   'vertex_insert_button',\n",
    "                   'vertex_remove_button',\n",
    "\n",
    "                   ]\n",
    "for btn in buttons_to_hide:\n",
    "    getattr(qctrl, btn).setEnabled(False)\n",
    "\n",
    "# Select the current, add tool for the points layer\n",
    "viewer.layers.selection.active = shapes_layer\n",
    "viewer.layers.selection.active.mode = 'pan_zoom'\n",
    "\n",
    "shapes_layer.events.data.connect(on_shapes_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/t95cmymn5cj5mmyw65m26t4w0000gn/T/ipykernel_19086/1020819677.py:76: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.6.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  qctrl = viewer.window.qt_viewer.controls.widgets[points_layer]\n"
     ]
    }
   ],
   "source": [
    "# Add the points layer to the viewer\n",
    "points_layer = viewer.add_points(None, \n",
    "                                 ndim=3,\n",
    "                                 name='Annotations', \n",
    "                                 scale=(1,1),\n",
    "                                 size=40,\n",
    "                                 border_color='dimgrey',\n",
    "                                 border_width=.2,\n",
    "                                 opacity=.6,\n",
    "                                 )\n",
    "# Store the initial length of the points data\n",
    "previous_length_points = len(points_layer.data)\n",
    "\n",
    "\n",
    "left_right_click = 'left'\n",
    "def on_mouse_press(layer, event):\n",
    "    '''\n",
    "    Generic function to catch left and right mouse clicks\n",
    "    '''\n",
    "    global left_right_click\n",
    "    if event.type == 'mouse_press':\n",
    "        if event.button == 1:  # Left-click\n",
    "            left_right_click = 'left'\n",
    "        elif event.button == 2:  # Right-click\n",
    "            left_right_click = 'right'     \n",
    "    \n",
    "\n",
    "def on_points_added(event):\n",
    "    '''\n",
    "    Function to run when points are added to the points layer\n",
    "    '''\n",
    "    \n",
    "    global points_layer\n",
    "    global labels_layer\n",
    "    global left_right_click\n",
    "    global prefetcher_worker\n",
    "    global previous_length_points\n",
    "    \n",
    "    current_length = len(points_layer.data)\n",
    "    if current_length > previous_length_points:\n",
    "        previous_length_points = current_length \n",
    "\n",
    "        # Execute prediction \n",
    "        newest_point_data =  points_layer.data[-1]\n",
    "        if left_right_click == 'left':\n",
    "            label = 1\n",
    "            points_layer.face_color[-1] = [0.59607846, 0.98431373, 0.59607846, 1.]\n",
    "            points_layer.symbol[-1] = 'o'\n",
    "        elif left_right_click == 'right':\n",
    "            label = 0\n",
    "            points_layer.face_color[-1] = [1., 1., 1., 1.]\n",
    "            points_layer.symbol[-1] = 'x'\n",
    "        points_layer.refresh() \n",
    "        # Run prediction\n",
    "        frame_idx  = int(newest_point_data[0])\n",
    "        point_data = newest_point_data[1:][::-1]\n",
    "        mask = run_new_pred(frame_idx=frame_idx,\n",
    "                            obj_id=0,\n",
    "                            label=label,\n",
    "                            point=point_data,\n",
    "                            )\n",
    "        labels_layer.data[frame_idx,:,:] = mask\n",
    "        labels_layer.refresh()   \n",
    "        \n",
    "        # Prefetch batch of images\n",
    "        # This is done here since adding it as direct mouse interaction \n",
    "        # slows down the first prediction\n",
    "        if not prefetcher_worker.is_running:\n",
    "            prefetcher_worker.run()\n",
    "\n",
    "\n",
    "points_layer.mouse_drag_callbacks.append(on_mouse_press)\n",
    "points_layer.events.data.connect(on_points_added)\n",
    "\n",
    "# Hide the transform, delete, and select buttons\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[points_layer]\n",
    "buttons_to_hide = ['transform_button', \n",
    "                   'delete_button', \n",
    "                   'select_button', \n",
    "]\n",
    "for btn in buttons_to_hide:\n",
    "    getattr(qctrl, btn).setEnabled(False)\n",
    "                   \n",
    "\n",
    "# Select the current, add tool for the points layer\n",
    "viewer.layers.selection.active = points_layer\n",
    "viewer.layers.selection.active.mode = 'add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Running mask prediction\n",
      "Running prefetcher\n",
      "6\n",
      "Running mask prediction\n",
      "Running prefetcher\n",
      "14\n",
      "Running mask prediction\n",
      "Running prefetcher\n"
     ]
    }
   ],
   "source": [
    "obj_id = 0\n",
    "\n",
    "\n",
    "@thread_worker\n",
    "def thread_predict(frame_idx, max_imgs):\n",
    "    global labels_layer\n",
    "\n",
    "    video_segments = {} \n",
    "    start_time = time.time()\n",
    "    # Prefetch images if they are not cached yet \n",
    "    _ = predictor.images[slice(frame_idx,frame_idx+max_imgs)]\n",
    "    \n",
    "    # Loop over frames and run prediction (single frame!)\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(start_frame_idx=frame_idx, \n",
    "                                                                                    max_frame_num_to_track=max_imgs):\n",
    "        \n",
    "        for i, out_obj_id in enumerate(out_obj_ids):\n",
    "            video_segments[out_frame_idx] = {out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()}\n",
    "            if not out_obj_id in predictor.inference_state['centroids']:\n",
    "                predictor.inference_state['centroids'][out_obj_id] = {}\n",
    "            if not out_obj_id in predictor.inference_state['areas']:\n",
    "                predictor.inference_state['areas'][out_obj_id] = {}\n",
    "                \n",
    "        # PICK ONE OBJ (OBJ_ID = 0 or whatever)\n",
    "        \n",
    "        #  Add the mask image as a new labels layer\n",
    "        mask = video_segments[out_frame_idx][obj_id] # THIS NEEDS TO BE MADE LAYER SPECIFIC \n",
    "        current_label = obj_id+1\n",
    "        if len(np.unique(mask))>1:\n",
    "            mask[mask==np.unique(mask)[1]] = current_label \n",
    "\n",
    "        mask = mask.squeeze()\n",
    "        props = measure.regionprops(mask.astype(int))[0]\n",
    "        predictor.inference_state['centroids'][obj_id][out_frame_idx] = props.centroid\n",
    "        predictor.inference_state['areas'][obj_id][out_frame_idx] = props.area\n",
    "        labels_layer.data[out_frame_idx,:,:] = mask\n",
    "        viewer.dims.set_point(0,out_frame_idx)\n",
    "        labels_layer.refresh()\n",
    "    end_time = time.time()\n",
    "    #print(f'start idx {frame_idx} | {max_imgs} frames in {end_time-start_time} s')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current chunk size: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:08<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f'Current chunk size: {chunk_size}')\n",
    "worker = thread_predict(frame_idx=viewer.dims.current_step[0], max_imgs=chunk_size) \n",
    "#worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "worker.start()  # start the thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tried to save the checkpoint, \n",
    "# # but this does not work. \n",
    "# # the check point model_state does not contain enough info \n",
    "# import torch\n",
    "# model_output_path = sample_dir / 'model_output.pth'    \n",
    "# torch.save({\n",
    "#             'model_state_dict': predictor.state_dict(),\n",
    "#             }, model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the whole video as test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest cached index 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:07<00:00,  2.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m prediction_worker\u001b[38;5;241m.\u001b[39mstart()  \n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHighest cached index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mnanmax(predictor\u001b[38;5;241m.\u001b[39mimages\u001b[38;5;241m.\u001b[39mcached_indices))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test \n",
    "for i in range(0,4000,chunk_size):\n",
    "    \n",
    "    prediction_worker = thread_predict(frame_idx=i, max_imgs=chunk_size)  \n",
    "    prediction_worker.setAutoDelete(True)\n",
    "    #worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "    prediction_worker.start()  \n",
    "    print(f'Highest cached index {int(np.nanmax(predictor.images.cached_indices))}')\n",
    "    time.sleep(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting\n",
    "# import seaborn as sns\n",
    "# sns.set_theme(style='white')\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "# from matplotlib import pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "# mpl.rcParams.update({\"axes.grid\" : True, \"grid.color\": \"grey\", \"grid.alpha\": .1})\n",
    "# plt.rcParams['xtick.major.size'] = 10\n",
    "# plt.rcParams['xtick.major.width'] = 1\n",
    "# plt.rcParams['ytick.major.size'] = 10\n",
    "# plt.rcParams['ytick.major.width'] = 1\n",
    "# plt.rcParams['xtick.bottom'] = True\n",
    "# plt.rcParams['ytick.left'] = True\n",
    "# mpl.rcParams['savefig.pad_inches'] = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the centroids over time\n",
    "# centroids = list(predictor.inference_state['centroids'][0].values())\n",
    "# centroids = np.stack(centroids)\n",
    "# areas = np.array(list(predictor.inference_state['areas'][0].values())).astype(float)\n",
    "# figure = plt.figure(figsize=(10,10))\n",
    "# plt.imshow(viewer.layers[0].data[0], cmap='gray')\n",
    "# #plt.plot(centroids[:,1], centroids[:,0], '-', color='k', alpha=.6)   \n",
    "# plt.scatter(centroids[:,1], centroids[:,0], s=areas/50, marker='.', color='pink', alpha=.15, lw=0)   \n",
    "# sns.despine(left=True,bottom=True)\n",
    "# plt.title(f'Centroids over time (n={centroids.shape[0]} frames)')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(predictor.inference_state['areas'][0].values()),'-', color='w', alpha=.6 )\n",
    "# plt.title('Area over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict_per_obj is huge \n",
    "# Structure\n",
    "# -> obj_id\n",
    "# --> cond_frame_outputs\n",
    "# --> non_cond_frame_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
