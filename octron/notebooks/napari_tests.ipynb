{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Napari tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "cur_path = Path(os.getcwd()).parent\n",
    "sam2_path = cur_path / 'sam2_octron'\n",
    "sys.path.append(cur_path.as_posix())\n",
    "from matplotlib import pyplot as plt\n",
    "import cmasher as cmr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='white')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "from napari.qt.threading import thread_worker\n",
    "from napari.utils import DirectLabelColormap\n",
    "from napari.utils.notifications import show_info\n",
    "import warnings\n",
    "warnings.simplefilter(action='always', category=FutureWarning)\n",
    "import time\n",
    "\n",
    "#### Importing additional stuff \n",
    "from skimage import measure\n",
    "from skimage.draw import polygon2mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Object organizer\n",
    "# from octron.sam2_octron.object_organizer import Obj, ObjectOrganizer\n",
    "# object_organizer = ObjectOrganizer()\n",
    "# object_organizer.add_entry(0, Obj(label='worm', suffix='0'))\n",
    "# object_organizer.add_entry(1, Obj(label='worm', suffix='1'))\n",
    "# object_organizer.add_entry(2, Obj(label='worm', suffix='two'))\n",
    "# object_organizer.add_entry(3, Obj(label='worm', suffix='three'))\n",
    "# object_organizer.add_entry(4, Obj(label='octopus', suffix=''))\n",
    "# object_organizer.add_entry(5, Obj(label='octopus', suffix='another'))\n",
    "# object_organizer.add_entry(6, Obj(label='octopus', suffix='three'))\n",
    "\n",
    "# object_organizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_path = '/Users/horst/Downloads/octron_project/worm masks.zip'\n",
    "# store = zarr.storage.ZipStore(zip_path, mode='w')\n",
    "\n",
    "# num_frames = 100\n",
    "# num_ch = 1\n",
    "# image_height = 512\n",
    "# image_width = 512\n",
    "# chunk_size = 10 \n",
    "# fill_value = 0\n",
    "# dtype = 'uint8'\n",
    "# image_zarr = zarr.create_array(store=store,\n",
    "#                                name='masks',\n",
    "#                                shape=(num_frames, num_ch, image_height, image_width),  \n",
    "#                                chunks=(chunk_size, num_ch, image_height, image_width), \n",
    "#                                fill_value=fill_value,\n",
    "#                                dtype=dtype,\n",
    "#                                overwrite=True,\n",
    "#                                       )\n",
    "# store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_path = Path('/Users/horst/Downloads/octron_project/water masks.zip')\n",
    "zip_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = zarr.storage.LocalStore(zip_path, read_only=False)  \n",
    "root = zarr.open_group(store=store, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing keys in zarr archive: ['masks']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Existing keys in zarr archive:\", list(root.array_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type               : Array\n",
       "Zarr format        : 3\n",
       "Data type          : DataType.uint8\n",
       "Shape              : (4067, 1000, 1000)\n",
       "Chunk shape        : (20, 1000, 1000)\n",
       "Order              : C\n",
       "Read-only          : False\n",
       "Store type         : LocalStore\n",
       "Filters            : ()\n",
       "Serializer         : BytesCodec(endian=<Endian.little: 'little'>)\n",
       "Compressors        : (ZstdCodec(level=0, checksum=False),)\n",
       "No. bytes          : 4067000000 (3.8G)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_zarr = root['masks']\n",
    "image_zarr.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the SAM2 model like you do in Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2_octron.helpers.sam2_checks import check_sam2_models\n",
    "from sam2_octron.helpers.sam2_octron import run_new_pred\n",
    "from sam2_octron.helpers.build_sam2_octron import build_sam2_octron\n",
    "from sam2_octron.helpers.sam2_zarr import create_image_zarr\n",
    "from sam2_octron.helpers.sam2_colors import create_label_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_yaml_path = sam2_path / 'models.yaml'\n",
    "models_dict = check_sam2_models(SAM2p1_BASE_URL='',\n",
    "                                       models_yaml_path=models_yaml_path,\n",
    "                                      force_download=False,\n",
    "                                      )\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Careful . these path descriptors differ slightly between notebook and \n",
    "# plugin version\n",
    "model = models_dict['sam2_large']\n",
    "\n",
    "config_path = Path(model['config_path'])\n",
    "checkpoint_path = sam2_path / Path(f\"{model['checkpoint_path']}\")\n",
    "predictor, device  = build_sam2_octron(config_file=config_path.as_posix(), \n",
    "                                       ckpt_path=checkpoint_path.as_posix(), \n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From napari, after loading video file, extract info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.dims.set_point(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video shape: (16012, 1258, 2048, 3)\n"
     ]
    }
   ],
   "source": [
    "num_frames, video_height, video_width, n_ch = viewer.layers[0].data.shape\n",
    "print(f\"Video shape: {viewer.layers[0].data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_edge = max(video_width, video_height) \n",
    "image_scaler = predictor.image_size / largest_edge\n",
    "resized_height = int(np.floor(image_scaler * video_height))\n",
    "resized_width = int(np.floor(image_scaler * video_width))\n",
    "assert max(resized_height, resized_width) == predictor.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2_octron.helpers.sam2_zarr import OctoZarr, Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "_resize_img = Resize(size=(resized_height, resized_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/horst/Documents/python/segment-anything-2/sam2/__init__.py'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam2.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0394736842105263"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "158/152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp output dir \n",
    "sample_dir = cur_path / 'sample_data'\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "sample_data_zarr = sample_dir / 'sample_data.zip'\n",
    "\n",
    "image_zarr = create_image_zarr(sample_data_zarr,\n",
    "                               num_frames=num_frames,\n",
    "                               image_height=predictor.image_size,\n",
    "                               chunk_size=chunk_size,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.init_state(video_data=video_data, zarr_store=image_zarr)\n",
    "predictor.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up thread worker to deal with prefetching batches of images\n",
    "@thread_worker\n",
    "def thread_prefetch_images(batch_size):\n",
    "    global viewer\n",
    "    current_indices = viewer.dims.current_step\n",
    "    print(f'Prefetching {batch_size} images, starting at frame {current_indices[0]}')\n",
    "    _ = predictor.images[slice(current_indices[0],current_indices[0]+batch_size)]\n",
    "prefetcher_worker = thread_prefetch_images(chunk_size)   \n",
    "prefetcher_worker.setAutoDelete(False)\n",
    "prefetcher_worker.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = create_label_colors(cmap='cmr.tropical')\n",
    "# Select colormap for labels layer based on category (label) and current object ID \n",
    "base_color = DirectLabelColormap(color_dict=colors[0], \n",
    "                                 use_selection=True, \n",
    "                                 selection=1,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement layer remove events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_layer_removed(event):\n",
    "    global viewer\n",
    "    global remove_current_layer, removed_layer\n",
    "    print('Calling on_layer_removed')\n",
    "    # if not remove_current_layer:\n",
    "    #     viewer.add_layer(removed_layer)\n",
    "    # else:\n",
    "    #     print(f\"Deleted layer {removed_layer}\")\n",
    "        \n",
    "def on_layer_removing(event):\n",
    "    global remove_current_layer, removed_layer\n",
    "    \n",
    "    layer2remove = event.source[event.index]\n",
    "    # Not sure if possible to delete more than one\n",
    "    # IF so, then take care of it ... event.sources is as list\n",
    "    \n",
    "    reply = QMessageBox.question(\n",
    "        None, \n",
    "        \"Confirmation\", \n",
    "        f\"Are you sure you want to delete layer\\n'{layer2remove}'\",\n",
    "        QMessageBox.Yes | QMessageBox.No,\n",
    "        QMessageBox.No\n",
    "    )\n",
    "    if reply == QMessageBox.No:\n",
    "        remove_current_layer = False\n",
    "        removed_layer = layer2remove\n",
    "    else:\n",
    "        remove_current_layer = True\n",
    "        \n",
    "viewer.layers.events.removing.connect(on_layer_removing)\n",
    "viewer.layers.events.removed.connect(on_layer_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_layer = viewer.add_labels(\n",
    "    image_layer['mask_dummy'], \n",
    "    name='SAM2 masks',  \n",
    "    opacity=0.4,  \n",
    "    blending='additive',  \n",
    "    colormap=base_color, \n",
    ")\n",
    "\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[labels_layer]\n",
    "buttons_to_hide = ['erase_button',\n",
    "                   'fill_button',\n",
    "                   'paint_button',\n",
    "                   'pick_button',\n",
    "                   'polygon_button',\n",
    "                   'transform_button',\n",
    "                   ]\n",
    "for btn in buttons_to_hide: \n",
    "    getattr(qctrl, btn).hide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapes layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = 'wormsy '\n",
    "label_name = label_name.strip().lower()\n",
    "current_color = colors[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the shapes layer to the viewer\n",
    "shapes_layer = viewer.add_shapes(None, \n",
    "                                 ndim=3,\n",
    "                                 name=label_name, \n",
    "                                 scale=(1,1),\n",
    "                                 edge_width=1,\n",
    "                                 edge_color=current_color,\n",
    "                                 face_color=[1,1,1,0],\n",
    "                                 opacity=.4,\n",
    "                                 )\n",
    "\n",
    "\n",
    "def on_shapes_changed(event):\n",
    "    global skip_event\n",
    "    action = event.action\n",
    "    if action in ['added','removed','changed']:\n",
    "        frame_idx = viewer.dims.current_step[0] \n",
    "        \n",
    "        if shapes_layer.mode == 'add_rectangle':\n",
    "            if action == 'removed':\n",
    "                return\n",
    "            # Take care of box input first. \n",
    "            # If the rectangle tool is selected, extract \"box\" coordinates\n",
    "            box = shapes_layer.data[-1]\n",
    "            if len(box) > 4:\n",
    "                box = box[-4:]\n",
    "            top_left, _, bottom_right, _ = box\n",
    "            top_left, bottom_right = top_left[1:], bottom_right[1:]\n",
    "            mask = run_new_pred(predictor=predictor,\n",
    "                                frame_idx=frame_idx,\n",
    "                                obj_id=0,\n",
    "                                labels=[1],\n",
    "                                box=[top_left[1],\n",
    "                                     top_left[0],\n",
    "                                     bottom_right[1],\n",
    "                                     bottom_right[0]\n",
    "                                     ],\n",
    "                                )\n",
    "            shapes_layer.data = shapes_layer.data[:-1]\n",
    "            shapes_layer.refresh()  \n",
    "        else:\n",
    "            # In all other cases, just treat shapes as masks \n",
    "            shape_mask = shapes_layer.to_masks((video_height, video_width))\n",
    "            shape_mask = np.sum(shape_mask, axis=0)\n",
    "            if not isinstance(shape_mask, np.ndarray):\n",
    "                return\n",
    "            shape_mask[shape_mask > 0] = 1\n",
    "            shape_mask = shape_mask.astype(np.uint8)\n",
    "        \n",
    "            label = 1 # Always positive for now\n",
    "            mask = run_new_pred(predictor=predictor,\n",
    "                                frame_idx=frame_idx,\n",
    "                                obj_id=0,\n",
    "                                labels=label,\n",
    "                                masks=shape_mask,\n",
    "                                )\n",
    "\n",
    "        labels_layer.data[frame_idx] = mask\n",
    "        labels_layer.refresh()\n",
    "        \n",
    "        # Prefetch batch of images\n",
    "        if not prefetcher_worker.is_running:\n",
    "            prefetcher_worker.run()\n",
    "    return\n",
    "\n",
    "# Store the initial length of the points data\n",
    "# previous_length_points = len(shapes_layer.data)\n",
    "# Hide the transform, delete, and select buttons\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[shapes_layer]\n",
    "buttons_to_hide = [\n",
    "                   'line_button',\n",
    "                   'path_button',\n",
    "                   'polyline_button',\n",
    "                   ]\n",
    "for btn in buttons_to_hide:\n",
    "    attr = getattr(qctrl, btn)\n",
    "    attr.hide()\n",
    "\n",
    "# Select the current, add tool for the points layer and attach the callback\n",
    "viewer.layers.selection.active = shapes_layer\n",
    "viewer.layers.selection.active.mode = 'pan_zoom'\n",
    "shapes_layer.events.data.connect(on_shapes_changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Points layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the points layer to the viewer\n",
    "points_layer = viewer.add_points(None, \n",
    "                                 ndim=3,\n",
    "                                 name=label_name, \n",
    "                                 scale=(1,1),\n",
    "                                 size=40,\n",
    "                                 border_color='dimgrey',\n",
    "                                 border_width=.2,\n",
    "                                 opacity=.6,\n",
    "                                 )\n",
    "\n",
    "\n",
    "left_right_click = 'left'\n",
    "def on_mouse_press(layer, event):\n",
    "    \"\"\"\n",
    "    Generic function to catch left and right mouse clicks\n",
    "    \"\"\"\n",
    "    global left_right_click\n",
    "    if event.type == 'mouse_press':\n",
    "        if event.button == 1:  # Left-click\n",
    "            left_right_click = 'left'\n",
    "        elif event.button == 2:  # Right-click\n",
    "            left_right_click = 'right'     \n",
    "    \n",
    "\n",
    "def on_points_changed(event):\n",
    "    \"\"\"\n",
    "    Function to run when points are added to the points layer\n",
    "    \"\"\"\n",
    "    action = event.action\n",
    "    frame_idx  = viewer.dims.current_step[0] \n",
    "    \n",
    "    left_positive_color  = [0.59607846, 0.98431373, 0.59607846, 1.]\n",
    "    right_negative_color = [1., 1., 1., 1.]\n",
    "    \n",
    "    if action == 'added':\n",
    "        # A new point has just been added. \n",
    "        # Find out if you are dealing with a left or right click    \n",
    "        if left_right_click == 'left':\n",
    "            label = 1\n",
    "            points_layer.face_color[-1] = left_positive_color\n",
    "            points_layer.symbol[-1] = 'o'\n",
    "        elif left_right_click == 'right':\n",
    "            label = 0\n",
    "            points_layer.face_color[-1] = right_negative_color\n",
    "            points_layer.symbol[-1] = 'x'\n",
    "        points_layer.refresh() # THIS IS IMPORTANT\n",
    "        # Prefetch batch of images\n",
    "        if not prefetcher_worker.is_running:\n",
    "            prefetcher_worker.run()\n",
    "        \n",
    "    # Loop through all the data and create points and labels\n",
    "    if action in ['added','removed','changed']:\n",
    "        labels = []\n",
    "        point_data = []\n",
    "        for pt_no, pt in enumerate(points_layer.data):\n",
    "            # Find out which label was attached to the point\n",
    "            # by going through the symbol lists\n",
    "            cur_symbol = points_layer.symbol[pt_no]\n",
    "            if cur_symbol in ['o','disc']:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            labels.append(label)\n",
    "            point_data.append(pt[1:][::-1]) # index 0 is the frame number\n",
    "            \n",
    "        # Then run the actual prediction\n",
    "        mask = run_new_pred(predictor=predictor,\n",
    "                            frame_idx=frame_idx,\n",
    "                            obj_id=0,\n",
    "                            labels=labels,\n",
    "                            points=point_data,\n",
    "                            )\n",
    "        labels_layer.data[frame_idx,:,:] = mask\n",
    "        labels_layer.refresh()   \n",
    "        \n",
    "    \n",
    "points_layer.mouse_drag_callbacks.append(on_mouse_press)\n",
    "points_layer.events.data.connect(on_points_changed)\n",
    "# Select the current, add tool for the points layer\n",
    "viewer.layers.selection.active = points_layer\n",
    "viewer.layers.selection.active.mode = 'add'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_id = 0\n",
    "\n",
    "@thread_worker\n",
    "def thread_predict(frame_idx, max_imgs):\n",
    "    global labels_layer\n",
    "\n",
    "    video_segments = {} \n",
    "    start_time = time.time()\n",
    "    # Prefetch images if they are not cached yet \n",
    "    _ = predictor.images[slice(frame_idx,frame_idx+max_imgs)]\n",
    "    \n",
    "    # Loop over frames and run prediction (single frame!)\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(start_frame_idx=frame_idx, \n",
    "                                                                                    max_frame_num_to_track=max_imgs):\n",
    "        \n",
    "        for i, out_obj_id in enumerate(out_obj_ids):\n",
    "            \n",
    "            torch_mask = out_mask_logits[i] > 0.0\n",
    "            out_mask = torch_mask.cpu().numpy()\n",
    "\n",
    "            video_segments[out_frame_idx] = {out_obj_id: out_mask}\n",
    "            if not out_obj_id in predictor.inference_state['centroids']:\n",
    "                predictor.inference_state['centroids'][out_obj_id] = {}\n",
    "            if not out_obj_id in predictor.inference_state['areas']:\n",
    "                predictor.inference_state['areas'][out_obj_id] = {}\n",
    "                \n",
    "        # PICK ONE OBJ (OBJ_ID = 0 or whatever)\n",
    "        \n",
    "        #  Add the mask image as a new labels layer\n",
    "        mask = video_segments[out_frame_idx][obj_id] # THIS NEEDS TO BE MADE LAYER SPECIFIC \n",
    "        current_label = obj_id+1\n",
    "        if len(np.unique(mask))>1:\n",
    "            mask[mask==np.unique(mask)[1]] = current_label \n",
    "\n",
    "        mask = mask.squeeze()\n",
    "        props = measure.regionprops(mask.astype(int))[0]\n",
    "        predictor.inference_state['centroids'][obj_id][out_frame_idx] = props.centroid\n",
    "        predictor.inference_state['areas'][obj_id][out_frame_idx] = props.area\n",
    "        labels_layer.data[out_frame_idx,:,:] = mask\n",
    "        viewer.dims.set_point(0,out_frame_idx)\n",
    "        labels_layer.refresh()\n",
    "    end_time = time.time()\n",
    "    #print(f'start idx {frame_idx} | {max_imgs} frames in {end_time-start_time} s')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(np.mean(labels_layer.data, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import tensor as torch_tensor\n",
    "# from skimage.morphology import disk\n",
    "\n",
    "# predictor.perform_morphological_operations = True\n",
    "\n",
    "# disk_size=10\n",
    "# compute_device=device\n",
    "# predictor.closing_kernel = torch_tensor(disk(disk_size).tolist()).to(compute_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Current chunk size: {chunk_size}')\n",
    "worker = thread_predict(frame_idx=viewer.dims.current_step[0], max_imgs=chunk_size) \n",
    "#worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "worker.start()  # start the thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tried to save the checkpoint, \n",
    "# # but this does not work. \n",
    "# # the check point model_state does not contain enough info \n",
    "# import torch\n",
    "# model_output_path = sample_dir / 'model_output.pth'    \n",
    "# torch.save({\n",
    "#             'model_state_dict': predictor.state_dict(),\n",
    "#             }, model_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Danger zone** Predict the whole video as test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test \n",
    "# for i in range(0,500,chunk_size):\n",
    "    \n",
    "#     prediction_worker = thread_predict(frame_idx=i, max_imgs=chunk_size)  \n",
    "#     prediction_worker.setAutoDelete(True)\n",
    "#     #worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "#     prediction_worker.start()  \n",
    "#     print(f'Highest cached index {int(np.nanmax(predictor.images.cached_indices))}')\n",
    "#     time.sleep(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting\n",
    "# import seaborn as sns\n",
    "# sns.set_theme(style='white')\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "# from matplotlib import pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "# mpl.rcParams.update({\"axes.grid\" : True, \"grid.color\": \"grey\", \"grid.alpha\": .1})\n",
    "# plt.rcParams['xtick.major.size'] = 10\n",
    "# plt.rcParams['xtick.major.width'] = 1\n",
    "# plt.rcParams['ytick.major.size'] = 10\n",
    "# plt.rcParams['ytick.major.width'] = 1\n",
    "# plt.rcParams['xtick.bottom'] = True\n",
    "# plt.rcParams['ytick.left'] = True\n",
    "# mpl.rcParams['savefig.pad_inches'] = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the centroids over time\n",
    "# centroids = list(predictor.inference_state['centroids'][0].values())\n",
    "# centroids = np.stack(centroids)\n",
    "# areas = np.array(list(predictor.inference_state['areas'][0].values())).astype(float)\n",
    "# figure = plt.figure(figsize=(10,10))\n",
    "# plt.imshow(viewer.layers[0].data[0], cmap='gray')\n",
    "# #plt.plot(centroids[:,1], centroids[:,0], '-', color='k', alpha=.6)   \n",
    "# plt.scatter(centroids[:,1], centroids[:,0], s=areas/50, marker='.', color='pink', alpha=.15, lw=0)   \n",
    "# sns.despine(left=True,bottom=True)\n",
    "# plt.title(f'Centroids over time (n={centroids.shape[0]} frames)')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(predictor.inference_state['areas'][0].values()),'-', color='w', alpha=.6 )\n",
    "# plt.title('Area over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict_per_obj is huge \n",
    "# Structure\n",
    "# -> obj_id\n",
    "# --> cond_frame_outputs\n",
    "# --> non_cond_frame_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
